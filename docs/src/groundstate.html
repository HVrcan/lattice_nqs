<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.4"/>
    <title>src.groundstate API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../src.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;src</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="function" href="#get_normalized_weights">get_normalized_weights</a>
            </li>
            <li>
                    <a class="function" href="#splice">splice</a>
            </li>
            <li>
                    <a class="function" href="#patch">patch</a>
            </li>
            <li>
                    <a class="class" href="#descent">descent</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#descent.__init__">descent</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.eta">eta</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.sampler">sampler</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.penalize_norm">penalize_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.Lnorm_importance">Lnorm_importance</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.geo">geo</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.refactoring">refactoring</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.states">states</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.loss">loss</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.psis">psis</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.normF">normF</a>
                        </li>
                        <li>
                                <a class="variable" href="#descent.normLnorm">normLnorm</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.geo_gradient">geo_gradient</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.gradient">gradient</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.compute_Lnorm">compute_Lnorm</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.numerical_norm_gradient">numerical_norm_gradient</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.normalise_params">normalise_params</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.refactor_Lnorm">refactor_Lnorm</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.lagrange_gradient">lagrange_gradient</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.optimize">optimize</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.process">process</a>
                        </li>
                        <li>
                                <a class="function" href="#descent.nudge">nudge</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../src.html">src</a><wbr>.groundstate    </h1>

                
                        <input id="mod-groundstate-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-groundstate-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">hilbert</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">full</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="k">def</span> <span class="nf">get_normalized_weights</span><span class="p">(</span><span class="n">psi</span><span class="p">):</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="sd">    Calculates the weights of the wave function `psi` when it&#39;s normalized.\\</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="sd">    Works only for the RBM architecture with `alpha=1/4` and `N=4`.</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="sd">    Returns the weights.</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a>    <span class="c1"># 1. normalize the wave function</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a>    <span class="n">psi</span> <span class="o">=</span> <span class="n">psi</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">psi</span><span class="p">)</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>    <span class="c1"># 2. calculate the thetas</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>    <span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arccosh</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">psi</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>    <span class="c1"># pseudoinverse</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>    <span class="n">invA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>    <span class="c1"># 3. get weights</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">invA</span> <span class="o">@</span> <span class="n">thetas</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>    <span class="c1"># get and return weights</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="k">def</span> <span class="nf">splice</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="sd">    Separates a complex-valued vector `w` into a double-dimensional real-valued vector.</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="sd">    - `w` -&gt; [`Re(w)`, `Im(w)`]</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="k">def</span> <span class="nf">patch</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="sd">    Rebuilds the complex vector that has been separated into a form given by the function `splice`.</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">    - `w` -&gt; `w[:len/2] + 1j*w[len/2:]`</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>    <span class="k">return</span> <span class="n">w</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="k">class</span> <span class="nc">descent</span><span class="p">:</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">    A gradient descent into the ground state of the lattice Heisenberg model.</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">    Variables</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">    =========</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">    - `lattice`: details about the spin lattice</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">    - `alpha`: network density</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">    - `eta`: learning rate</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">    - `tinv`: is the network translationally invariant? this changes the number of used parameters</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">    - `afm_sector`: keep only the zero-magnetization sector of the Hiblert space?</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">    - `scale`: multiply the wave function with this number</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">    - `taming`: do you want to rescale the gradient(s)? this helps if they are too huge</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">    - `real_parameters`: initialize parameters as real?</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">    - `epsilon`**\***: a function depending on the step that modifies the regularization during training</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">    - `normalize_initial`: how do you want to recalculate weights to normalize the wave fucntion? choose: `&#39;analytical&#39;`, `&#39;numerical&#39;`, or leave default `None`.</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">    - `penalize_norm`: do you want to penalize the norm during training?</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="sd">    - `Lnorm_importance`**\***: a function that modifies the learning rate of normalization during training</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="sd">    - `lagrange`: decides whether to use lagrange multipliers to conserve the norm during training. **NB**: setting to `True` normalizes the initial parameters with `&#39;tdvp&#39;`, and overwrites `refactoring` and `penalize_norm` to `False`.</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a><span class="sd">    **\*Both of these must be functions. If you don&#39;t know what they mean, leave them as defaults.**</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="sd">    Properties</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="sd">    =========</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="sd">    - `sampler`: everything regarding the full sampling</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="sd">    - `states`: list of network parameter states</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a><span class="sd">    - `loss`: list of loss function values (variational energy)</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a><span class="sd">    - `psis`: list of wave function vectors</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a><span class="sd">    - `normF`, `normSF`, `normLnorm`: list of norms of gradients of F, S^-1F, L_norm respectively</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a><span class="sd">    </span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="sd">    Methods</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="sd">    =========</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="sd">    - `gradient`: calculates gradient with stochastic reconfiguration</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="sd">    - `optimize`: performs the descent</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="sd">    - `process (links)`: calculates the optimization curve for the (z)correlation function(s) at the given `links` list</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="sd">    Regularizaiton recipe taken from the Carleo &amp; Troyer paper.</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lattice</span><span class="p">:</span><span class="n">hilbert</span><span class="o">.</span><span class="n">lattice</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span>  
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>                 <span class="n">regularization</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="mf">0.99</span><span class="o">**</span><span class="n">s</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">),</span> <span class="c1"># regulator?</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>                 <span class="n">phase</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>                 <span class="n">tinv</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">normal_sampling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># invariance and sampling details </span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>                 <span class="n">geometric</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># use the geometric method that i coded just for fun?</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>                 <span class="n">afm_sector</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># restrict to antiferromagnetic sector?</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>                 <span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;heisenberg&#39;</span><span class="p">,</span> <span class="c1"># which model?</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>                 <span class="n">gauge</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>                 <span class="n">real_parameters</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>                 <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>                 <span class="n">refactoring</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>                 <span class="n">normalize_initial</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>                 <span class="n">penalize_norm</span> <span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>                 <span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>                 <span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>        
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span> <span class="c1"># learning rate</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">full</span><span class="o">.</span><span class="n">sampler</span><span class="p">(</span><span class="n">hilbert</span><span class="o">.</span><span class="n">space</span><span class="p">(</span><span class="n">lattice</span><span class="p">,</span> <span class="n">afm</span><span class="o">=</span><span class="n">afm_sector</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">gauge</span><span class="o">=</span><span class="n">gauge</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> 
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>                                    <span class="n">tinvariant</span><span class="o">=</span><span class="n">tinv</span><span class="p">,</span> <span class="n">normal</span><span class="o">=</span><span class="n">normal_sampling</span><span class="p">,</span> <span class="n">real</span><span class="o">=</span><span class="n">real_parameters</span><span class="p">,</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>                                    <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span> <span class="c1"># sampler containig the entire Hilbert space and the network</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="c1"># get normalized funciton weights if you want to normalize initial state</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>        <span class="c1">#NOTE: this part is a bit obsolete now, but I&#39;m still leaving it for posterity</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>        <span class="k">if</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;exact&#39;</span><span class="p">:</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="c1"># this should update the sampler and all of its properties, including the parameters, the energy, and the psi</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">get_normalized_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>        <span class="k">elif</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;tdvp&#39;</span><span class="p">:</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>   
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="n">penalize_norm</span> <span class="c1"># do you want to penalize the norm during training?</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="n">Lnorm_importance</span> <span class="c1"># how important is the norm loss?</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>        <span class="c1"># regularization</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">regularization</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="c1"># regulator value</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>        <span class="c1"># geometric?</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">geo</span> <span class="o">=</span> <span class="n">geometric</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>        <span class="c1">#taming</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="n">refactoring</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>        <span class="c1"># use lagrange multipliers?</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>        <span class="k">if</span> <span class="n">lagrange</span><span class="p">:</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>        <span class="c1"># trackable quantities</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="c1">#self.states = [self.sampler.network.parameters.all * np.exp(1j*phase)] # list with all the states of the proccess</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="c1">#TODO</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase</span><span class="p">)]</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">]</span> <span class="c1"># values of the loss function</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">psis</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>        <span class="c1"># more trackable</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normF</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">)]</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">))]</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>    <span class="k">def</span> <span class="nf">geo_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="sd">        Calculates the gradient using the geometric implementation.</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>        <span class="c1"># Quantum Fisher Matrix and the energy gradient</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">))</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>        <span class="c1"># split them into real parametrization</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">S</span><span class="p">,[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="n">j</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">F</span><span class="p">,[</span><span class="mf">1.</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">])</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>        <span class="n">metric</span> <span class="o">=</span> <span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">rcond</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># solve the problem</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># get back into the complex</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>        <span class="k">return</span> <span class="n">res</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a><span class="sd">        Calculates the analytical gradient of the loss function.\\</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a><span class="sd">        This is obtained by the &#39;stochastic reconfiguration method&#39; - check litterature.</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a><span class="sd">        UPDATE THIS:</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a><span class="sd">        The gradient itself is obtained convergently, through the `np.linalg.lstsq` function (rather than regularization and pseudoinversion).</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>        <span class="c1">#return np.linalg.lstsq(self.sampler.S, -self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>        <span class="c1">#return self.sampler.F</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo</span><span class="p">:</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>            <span class="c1">#return np.linalg.lstsq(self.sampler.S + self.epsilon(s)*np.eye(len(self.sampler.S)), self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo_gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>    <span class="c1">#region normalizing </span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>    <span class="c1">#NOTE: this part is a bit obsolete now, but I&#39;m still leaving it for posterity</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>    <span class="k">def</span> <span class="nf">compute_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loss for norm optimization: `L=(1-&lt;psi|psi&gt;)^2.` &quot;&quot;&quot;</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>        <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>    
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>    <span class="k">def</span> <span class="nf">numerical_norm_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a><span class="sd">        Compute the numerical gradient of L_norm with respect to w.\\</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a><span class="sd">        Uses the derivative scheme from Wirtinger calculus. See Torch documentation.</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a><span class="sd">        - `epsilon`: infinitesimal step in parameters</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>        <span class="n">w</span> <span class="o">=</span> <span class="n">splice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">all</span><span class="p">)</span> <span class="c1"># save weights</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>        <span class="n">w_patched</span> <span class="o">=</span> <span class="n">patch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>        
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">)):</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>            <span class="c1"># calculate positive- and negative-moved *spliced* weights</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>            <span class="c1"># not the most optimal way to do things, thank ChatGPT for that</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>            <span class="n">w_pos</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>            <span class="n">w_neg</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>            <span class="n">w_pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>            <span class="n">w_neg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>            <span class="c1"># update NN weights (*patched* ofc) and calculate the loss for both positive and negative</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_pos</span><span class="p">))</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>            <span class="n">loss_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_neg</span><span class="p">))</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>            <span class="n">loss_neg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>            
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>            <span class="c1"># restore the original *patched* weights after computation</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w_patched</span><span class="p">)</span> 
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>            <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_pos</span> <span class="o">-</span> <span class="n">loss_neg</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="p">)</span>  <span class="c1"># Central difference approximation</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>        <span class="k">return</span> <span class="n">patch</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">*</span><span class="mf">0.5</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>    
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>    <span class="k">def</span> <span class="nf">normalise_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a><span class="sd">        Attempts to find a set of network parameters which represent a normalised wave function.\\</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a><span class="sd">        This is done by optimizing the loss `L = (1-&lt;psi|psi&gt;)**2` with gradient descent.</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a><span class="sd">        - `tolerance`: controls how precise the convergence should be</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="sd">        - `maxiters`: manual break for the training loop if it goes into too many steps</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a><span class="sd">        Fun fact: normaliSe is British, normaliZe is American English</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>        <span class="c1"># counter for manual loop break</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>        <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">:</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>            <span class="c1"># 1. calculate the gradient</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>            <span class="c1">#grad = self.numerical_norm_gradient()</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>            <span class="c1"># 3. update weights by gradient descent</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>                                <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>            <span class="c1"># 4. breaking conditions</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>            <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>            <span class="k">if</span> <span class="n">iters</span> <span class="o">&gt;=</span> <span class="n">maxiters</span><span class="p">:</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loop broken after &quot;</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="s2">&quot; iterations&quot;</span><span class="p">)</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>                <span class="k">break</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parametric normalization complete, norm = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>        
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>    <span class="k">def</span> <span class="nf">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradLnorm</span><span class="p">):</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a><span class="sd">        Rescales the `gradLnorm` so that its norm has an upper limit corresponding to the maximum norm of the energy gradient in the whole simulation.\\</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a><span class="sd">        This means that, if its norm is bigger, then reduce it to that limit, otherwise do nothing.</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>        <span class="c1">#NOTE: garbage code</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>            <span class="c1">#self.normSF.append(np.linalg.norm(self.gradient() / (1. + self.eta*np.linalg.norm(self.gradient())) ) if self.taming else np.linalg.norm(self.gradient()))</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad / (1. + self.eta*np.linalg.norm(self.sampler.Lnorm_grad)) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(gradLnorm * np.linalg.norm(self.sampler.F) / np.linalg.norm(gradLnorm) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad * max(self.normF) / np.linalg.norm(self.sampler.Lnorm_grad) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>            
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>            <span class="c1">#grad = grad / (1 + self.eta*np.linalg.norm(grad))</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>            <span class="c1">#gradLnorm = gradLnorm / (1 + self.eta*np.linalg.norm(gradLnorm))</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>            <span class="c1">#gradLnorm = gradLnorm * max(self.normF) / np.linalg.norm(gradLnorm)</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>        <span class="c1"># hopefully you skipped this</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>        <span class="c1"># upper limit to the norm</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>        <span class="n">upper_limit</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="p">)</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>        <span class="c1">#upper_limit= self.normF[-1]</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>        <span class="c1">#gradLnorm = np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S))) @ gradLnorm</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>        <span class="c1"># see if it&#39;s bigger</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">upper_limit</span><span class="p">:</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="n">upper_limit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>        <span class="c1">#return np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S)))@gradLnorm</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>        <span class="k">return</span> <span class="n">gradLnorm</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>    <span class="c1">#endregion</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>    <span class="k">def</span> <span class="nf">lagrange_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a><span class="sd">        Performs an update of parameters that conserves the norm using Lagrange multipler(s).</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>        <span class="c1"># LHS of the EoM</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>        <span class="c1"># make the matrix</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>        <span class="c1"># apply the regularization only on the S matrix?</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)),</span> 
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>                             <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>                             <span class="p">))</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a>        <span class="c1"># M = np.column_stack((self.sampler.S, </span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>        <span class="c1">#                      self.sampler.Lnorm_grad))</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">,</span> <span class="mf">0.</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>            <span class="p">)))</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>        <span class="c1"># or regularization of the WHOLE matrix?</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>        <span class="c1"># M = M + self.epsilon(s)*np.eye(len(M))</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>        
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>        <span class="c1"># RHS of the EoM</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>        <span class="c1"># solution</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="c1"># don&#39;t forget to leave out the last element because that&#39;s the multipler</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">@</span> <span class="n">F</span><span class="p">)</span> <span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">precision</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">miniter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a><span class="sd">        Performs the gradient descent algorithm.</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a><span class="sd">        - `precision`: convergence criterion for the loss function</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a><span class="sd">        - `miniter`: minimum number of iterations</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">        - `maxiter`: maximum number of iterations</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>        <span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># iterations</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>        <span class="c1">#params = np.real(self.states[0].copy()) + 1j*np.zeros(len(self.states[0])) # initial parameters</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>        <span class="c1"># main loop</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>        <span class="k">while</span> <span class="nb">iter</span> <span class="o">&lt;=</span> <span class="n">maxiter</span><span class="p">:</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>            <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>            <span class="c1"># if self.zigzag:</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>            <span class="c1">#     self.normalise_params()</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="nb">iter</span><span class="p">)</span> <span class="c1"># gradient</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>            <span class="c1">#do you refactor?</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span><span class="p">:</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>                <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>            <span class="c1">#do you penalize?</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span><span class="p">:</span> <span class="c1"># if you&#39;re also normalizing, penalize the norm in the gradient</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>                <span class="n">grad</span> <span class="o">+=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>            <span class="c1"># if not either of those things, you are probably lagranging</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span><span class="p">:</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>                <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange_gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">)</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>            <span class="n">params</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># descent</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="c1"># change all the elements of the sampler (network, S, F, etc...)</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>            <span class="c1"># accumulate things</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span> <span class="c1"># append the loss function</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">psis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>            <span class="c1">#self.normF.append(np.linalg.norm(self.sampler.F))</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">)))</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>            
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>            <span class="c1"># sponsored messages</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">iter</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="s2">&quot;... energy: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>            
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>            <span class="c1"># check criteria</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>            <span class="k">if</span> <span class="nb">iter</span> <span class="o">&gt;</span> <span class="n">miniter</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">precision</span><span class="p">:</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>                <span class="k">break</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>        
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>    
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">links</span><span class="p">):</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a><span class="sd">        Calculates the correlation functions at each state of the process.\\</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a><span class="sd">        Correlatoins are written in a `link -&gt; list` dictionary, where the `list` is all the values of the correlation at the lattice `link` during the process.</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a><span class="sd">        - `links`: an array of tuples representing links on the lattice where the calculation is performed </span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>        <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculating correlation at link&quot;</span><span class="p">,</span> <span class="n">link</span><span class="p">)</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">))</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>                <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Cvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Czvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>                    
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>                    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>    
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>    <span class="k">def</span> <span class="nf">nudge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase_nudge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">):</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a><span class="sd">        Corrects the weights of the neural network to be complex, but still have the same energy.\\</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a><span class="sd">        Uses a root-finding algorithm to find the state whose energy offset is zero. This is equivalent to supervised learning.</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a><span class="sd">        1. Initialize weights: take the trained weights in `states[-1]` and multiply them with `exp(1j*phase_nudge)`.</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a><span class="sd">        2. Find the root: construct a function `L (w) = |E_0 - E(w)|`, where `E_0` is the pre-trained energy, and find the weights `w` for which this function has a root.</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a><span class="sd">        USE only if you have performed the graident descent by the `descent()` function. Otherwise the results are senseless.</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>        <span class="c1"># 1. nudge the parameters</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>        <span class="n">w0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase_nudge</span><span class="p">)</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>        <span class="c1"># 2. create the function</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>        <span class="k">def</span> <span class="nf">nudgeloss</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>            <span class="c1"># update the sampler</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">just_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>            <span class="c1"># return the value</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>        
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>        <span class="c1"># 3. run the root finder</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>        <span class="n">nudged_weights</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">newton</span><span class="p">(</span><span class="n">nudgeloss</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>        <span class="k">return</span> <span class="n">nudged_weights</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>            
</span></pre></div>


            </section>
                <section id="get_normalized_weights">
                            <input id="get_normalized_weights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_normalized_weights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">psi</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="get_normalized_weights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_normalized_weights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_normalized_weights-9"><a href="#get_normalized_weights-9"><span class="linenos"> 9</span></a><span class="k">def</span> <span class="nf">get_normalized_weights</span><span class="p">(</span><span class="n">psi</span><span class="p">):</span>
</span><span id="get_normalized_weights-10"><a href="#get_normalized_weights-10"><span class="linenos">10</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="get_normalized_weights-11"><a href="#get_normalized_weights-11"><span class="linenos">11</span></a><span class="sd">    Calculates the weights of the wave function `psi` when it&#39;s normalized.\\</span>
</span><span id="get_normalized_weights-12"><a href="#get_normalized_weights-12"><span class="linenos">12</span></a><span class="sd">    Works only for the RBM architecture with `alpha=1/4` and `N=4`.</span>
</span><span id="get_normalized_weights-13"><a href="#get_normalized_weights-13"><span class="linenos">13</span></a>
</span><span id="get_normalized_weights-14"><a href="#get_normalized_weights-14"><span class="linenos">14</span></a><span class="sd">    Returns the weights.</span>
</span><span id="get_normalized_weights-15"><a href="#get_normalized_weights-15"><span class="linenos">15</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_normalized_weights-16"><a href="#get_normalized_weights-16"><span class="linenos">16</span></a>    <span class="c1"># 1. normalize the wave function</span>
</span><span id="get_normalized_weights-17"><a href="#get_normalized_weights-17"><span class="linenos">17</span></a>    <span class="n">psi</span> <span class="o">=</span> <span class="n">psi</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">psi</span><span class="p">)</span>
</span><span id="get_normalized_weights-18"><a href="#get_normalized_weights-18"><span class="linenos">18</span></a>
</span><span id="get_normalized_weights-19"><a href="#get_normalized_weights-19"><span class="linenos">19</span></a>    <span class="c1"># 2. calculate the thetas</span>
</span><span id="get_normalized_weights-20"><a href="#get_normalized_weights-20"><span class="linenos">20</span></a>    <span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arccosh</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">psi</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="get_normalized_weights-21"><a href="#get_normalized_weights-21"><span class="linenos">21</span></a>
</span><span id="get_normalized_weights-22"><a href="#get_normalized_weights-22"><span class="linenos">22</span></a>    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span id="get_normalized_weights-23"><a href="#get_normalized_weights-23"><span class="linenos">23</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
</span><span id="get_normalized_weights-24"><a href="#get_normalized_weights-24"><span class="linenos">24</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
</span><span id="get_normalized_weights-25"><a href="#get_normalized_weights-25"><span class="linenos">25</span></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</span><span id="get_normalized_weights-26"><a href="#get_normalized_weights-26"><span class="linenos">26</span></a>    <span class="c1"># pseudoinverse</span>
</span><span id="get_normalized_weights-27"><a href="#get_normalized_weights-27"><span class="linenos">27</span></a>    <span class="n">invA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="get_normalized_weights-28"><a href="#get_normalized_weights-28"><span class="linenos">28</span></a>
</span><span id="get_normalized_weights-29"><a href="#get_normalized_weights-29"><span class="linenos">29</span></a>    <span class="c1"># 3. get weights</span>
</span><span id="get_normalized_weights-30"><a href="#get_normalized_weights-30"><span class="linenos">30</span></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">invA</span> <span class="o">@</span> <span class="n">thetas</span>
</span><span id="get_normalized_weights-31"><a href="#get_normalized_weights-31"><span class="linenos">31</span></a>
</span><span id="get_normalized_weights-32"><a href="#get_normalized_weights-32"><span class="linenos">32</span></a>    <span class="c1"># get and return weights</span>
</span><span id="get_normalized_weights-33"><a href="#get_normalized_weights-33"><span class="linenos">33</span></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Calculates the weights of the wave function <code>psi</code> when it's normalized.\
Works only for the RBM architecture with <code>alpha=1/4</code> and <code>N=4</code>.</p>

<p>Returns the weights.</p>
</div>


                </section>
                <section id="splice">
                            <input id="splice-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">splice</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">w</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="splice-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#splice"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="splice-35"><a href="#splice-35"><span class="linenos">35</span></a><span class="k">def</span> <span class="nf">splice</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="splice-36"><a href="#splice-36"><span class="linenos">36</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="splice-37"><a href="#splice-37"><span class="linenos">37</span></a><span class="sd">    Separates a complex-valued vector `w` into a double-dimensional real-valued vector.</span>
</span><span id="splice-38"><a href="#splice-38"><span class="linenos">38</span></a><span class="sd">    - `w` -&gt; [`Re(w)`, `Im(w)`]</span>
</span><span id="splice-39"><a href="#splice-39"><span class="linenos">39</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="splice-40"><a href="#splice-40"><span class="linenos">40</span></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</span></pre></div>


            <div class="docstring"><p>Separates a complex-valued vector <code>w</code> into a double-dimensional real-valued vector.</p>

<ul>
<li><code>w</code> -> [<code>Re(w)</code>, <code>Im(w)</code>]</li>
</ul>
</div>


                </section>
                <section id="patch">
                            <input id="patch-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">patch</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">w</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="patch-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#patch"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="patch-42"><a href="#patch-42"><span class="linenos">42</span></a><span class="k">def</span> <span class="nf">patch</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="patch-43"><a href="#patch-43"><span class="linenos">43</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="patch-44"><a href="#patch-44"><span class="linenos">44</span></a><span class="sd">    Rebuilds the complex vector that has been separated into a form given by the function `splice`.</span>
</span><span id="patch-45"><a href="#patch-45"><span class="linenos">45</span></a><span class="sd">    - `w` -&gt; `w[:len/2] + 1j*w[len/2:]`</span>
</span><span id="patch-46"><a href="#patch-46"><span class="linenos">46</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="patch-47"><a href="#patch-47"><span class="linenos">47</span></a>    <span class="k">return</span> <span class="n">w</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span>
</span></pre></div>


            <div class="docstring"><p>Rebuilds the complex vector that has been separated into a form given by the function <code><a href="#splice">splice</a></code>.</p>

<ul>
<li><code>w</code> -> <code>w[:len/2] + 1j*w[len/2:]</code></li>
</ul>
</div>


                </section>
                <section id="descent">
                            <input id="descent-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">descent</span>:

                <label class="view-source-button" for="descent-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent-50"><a href="#descent-50"><span class="linenos"> 50</span></a><span class="k">class</span> <span class="nc">descent</span><span class="p">:</span>
</span><span id="descent-51"><a href="#descent-51"><span class="linenos"> 51</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-52"><a href="#descent-52"><span class="linenos"> 52</span></a><span class="sd">    A gradient descent into the ground state of the lattice Heisenberg model.</span>
</span><span id="descent-53"><a href="#descent-53"><span class="linenos"> 53</span></a>
</span><span id="descent-54"><a href="#descent-54"><span class="linenos"> 54</span></a><span class="sd">    Variables</span>
</span><span id="descent-55"><a href="#descent-55"><span class="linenos"> 55</span></a><span class="sd">    =========</span>
</span><span id="descent-56"><a href="#descent-56"><span class="linenos"> 56</span></a><span class="sd">    - `lattice`: details about the spin lattice</span>
</span><span id="descent-57"><a href="#descent-57"><span class="linenos"> 57</span></a><span class="sd">    - `alpha`: network density</span>
</span><span id="descent-58"><a href="#descent-58"><span class="linenos"> 58</span></a><span class="sd">    - `eta`: learning rate</span>
</span><span id="descent-59"><a href="#descent-59"><span class="linenos"> 59</span></a><span class="sd">    - `tinv`: is the network translationally invariant? this changes the number of used parameters</span>
</span><span id="descent-60"><a href="#descent-60"><span class="linenos"> 60</span></a><span class="sd">    - `afm_sector`: keep only the zero-magnetization sector of the Hiblert space?</span>
</span><span id="descent-61"><a href="#descent-61"><span class="linenos"> 61</span></a><span class="sd">    - `scale`: multiply the wave function with this number</span>
</span><span id="descent-62"><a href="#descent-62"><span class="linenos"> 62</span></a><span class="sd">    - `taming`: do you want to rescale the gradient(s)? this helps if they are too huge</span>
</span><span id="descent-63"><a href="#descent-63"><span class="linenos"> 63</span></a><span class="sd">    - `real_parameters`: initialize parameters as real?</span>
</span><span id="descent-64"><a href="#descent-64"><span class="linenos"> 64</span></a><span class="sd">    - `epsilon`**\***: a function depending on the step that modifies the regularization during training</span>
</span><span id="descent-65"><a href="#descent-65"><span class="linenos"> 65</span></a><span class="sd">    - `normalize_initial`: how do you want to recalculate weights to normalize the wave fucntion? choose: `&#39;analytical&#39;`, `&#39;numerical&#39;`, or leave default `None`.</span>
</span><span id="descent-66"><a href="#descent-66"><span class="linenos"> 66</span></a><span class="sd">    - `penalize_norm`: do you want to penalize the norm during training?</span>
</span><span id="descent-67"><a href="#descent-67"><span class="linenos"> 67</span></a><span class="sd">    - `Lnorm_importance`**\***: a function that modifies the learning rate of normalization during training</span>
</span><span id="descent-68"><a href="#descent-68"><span class="linenos"> 68</span></a><span class="sd">    - `lagrange`: decides whether to use lagrange multipliers to conserve the norm during training. **NB**: setting to `True` normalizes the initial parameters with `&#39;tdvp&#39;`, and overwrites `refactoring` and `penalize_norm` to `False`.</span>
</span><span id="descent-69"><a href="#descent-69"><span class="linenos"> 69</span></a>
</span><span id="descent-70"><a href="#descent-70"><span class="linenos"> 70</span></a><span class="sd">    **\*Both of these must be functions. If you don&#39;t know what they mean, leave them as defaults.**</span>
</span><span id="descent-71"><a href="#descent-71"><span class="linenos"> 71</span></a>
</span><span id="descent-72"><a href="#descent-72"><span class="linenos"> 72</span></a><span class="sd">    Properties</span>
</span><span id="descent-73"><a href="#descent-73"><span class="linenos"> 73</span></a><span class="sd">    =========</span>
</span><span id="descent-74"><a href="#descent-74"><span class="linenos"> 74</span></a><span class="sd">    - `sampler`: everything regarding the full sampling</span>
</span><span id="descent-75"><a href="#descent-75"><span class="linenos"> 75</span></a><span class="sd">    - `states`: list of network parameter states</span>
</span><span id="descent-76"><a href="#descent-76"><span class="linenos"> 76</span></a><span class="sd">    - `loss`: list of loss function values (variational energy)</span>
</span><span id="descent-77"><a href="#descent-77"><span class="linenos"> 77</span></a><span class="sd">    - `psis`: list of wave function vectors</span>
</span><span id="descent-78"><a href="#descent-78"><span class="linenos"> 78</span></a><span class="sd">    - `normF`, `normSF`, `normLnorm`: list of norms of gradients of F, S^-1F, L_norm respectively</span>
</span><span id="descent-79"><a href="#descent-79"><span class="linenos"> 79</span></a><span class="sd">    </span>
</span><span id="descent-80"><a href="#descent-80"><span class="linenos"> 80</span></a><span class="sd">    Methods</span>
</span><span id="descent-81"><a href="#descent-81"><span class="linenos"> 81</span></a><span class="sd">    =========</span>
</span><span id="descent-82"><a href="#descent-82"><span class="linenos"> 82</span></a><span class="sd">    - `gradient`: calculates gradient with stochastic reconfiguration</span>
</span><span id="descent-83"><a href="#descent-83"><span class="linenos"> 83</span></a><span class="sd">    - `optimize`: performs the descent</span>
</span><span id="descent-84"><a href="#descent-84"><span class="linenos"> 84</span></a><span class="sd">    - `process (links)`: calculates the optimization curve for the (z)correlation function(s) at the given `links` list</span>
</span><span id="descent-85"><a href="#descent-85"><span class="linenos"> 85</span></a>
</span><span id="descent-86"><a href="#descent-86"><span class="linenos"> 86</span></a><span class="sd">    Regularizaiton recipe taken from the Carleo &amp; Troyer paper.</span>
</span><span id="descent-87"><a href="#descent-87"><span class="linenos"> 87</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="descent-88"><a href="#descent-88"><span class="linenos"> 88</span></a>
</span><span id="descent-89"><a href="#descent-89"><span class="linenos"> 89</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lattice</span><span class="p">:</span><span class="n">hilbert</span><span class="o">.</span><span class="n">lattice</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span>  
</span><span id="descent-90"><a href="#descent-90"><span class="linenos"> 90</span></a>                 <span class="n">regularization</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="mf">0.99</span><span class="o">**</span><span class="n">s</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">),</span> <span class="c1"># regulator?</span>
</span><span id="descent-91"><a href="#descent-91"><span class="linenos"> 91</span></a>                 <span class="n">phase</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
</span><span id="descent-92"><a href="#descent-92"><span class="linenos"> 92</span></a>                 <span class="n">tinv</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">normal_sampling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># invariance and sampling details </span>
</span><span id="descent-93"><a href="#descent-93"><span class="linenos"> 93</span></a>                 <span class="n">geometric</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># use the geometric method that i coded just for fun?</span>
</span><span id="descent-94"><a href="#descent-94"><span class="linenos"> 94</span></a>                 <span class="n">afm_sector</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># restrict to antiferromagnetic sector?</span>
</span><span id="descent-95"><a href="#descent-95"><span class="linenos"> 95</span></a>                 <span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;heisenberg&#39;</span><span class="p">,</span> <span class="c1"># which model?</span>
</span><span id="descent-96"><a href="#descent-96"><span class="linenos"> 96</span></a>                 <span class="n">gauge</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent-97"><a href="#descent-97"><span class="linenos"> 97</span></a>                 <span class="n">real_parameters</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent-98"><a href="#descent-98"><span class="linenos"> 98</span></a>                 <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="descent-99"><a href="#descent-99"><span class="linenos"> 99</span></a>                 <span class="n">refactoring</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent-100"><a href="#descent-100"><span class="linenos">100</span></a>                 <span class="n">normalize_initial</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="descent-101"><a href="#descent-101"><span class="linenos">101</span></a>                 <span class="n">penalize_norm</span> <span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="descent-102"><a href="#descent-102"><span class="linenos">102</span></a>                 <span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="descent-103"><a href="#descent-103"><span class="linenos">103</span></a>                 <span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent-104"><a href="#descent-104"><span class="linenos">104</span></a>                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="descent-105"><a href="#descent-105"><span class="linenos">105</span></a>        
</span><span id="descent-106"><a href="#descent-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span> <span class="c1"># learning rate</span>
</span><span id="descent-107"><a href="#descent-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">full</span><span class="o">.</span><span class="n">sampler</span><span class="p">(</span><span class="n">hilbert</span><span class="o">.</span><span class="n">space</span><span class="p">(</span><span class="n">lattice</span><span class="p">,</span> <span class="n">afm</span><span class="o">=</span><span class="n">afm_sector</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">gauge</span><span class="o">=</span><span class="n">gauge</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> 
</span><span id="descent-108"><a href="#descent-108"><span class="linenos">108</span></a>                                    <span class="n">tinvariant</span><span class="o">=</span><span class="n">tinv</span><span class="p">,</span> <span class="n">normal</span><span class="o">=</span><span class="n">normal_sampling</span><span class="p">,</span> <span class="n">real</span><span class="o">=</span><span class="n">real_parameters</span><span class="p">,</span>
</span><span id="descent-109"><a href="#descent-109"><span class="linenos">109</span></a>                                    <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span> <span class="c1"># sampler containig the entire Hilbert space and the network</span>
</span><span id="descent-110"><a href="#descent-110"><span class="linenos">110</span></a>
</span><span id="descent-111"><a href="#descent-111"><span class="linenos">111</span></a>        <span class="c1"># get normalized funciton weights if you want to normalize initial state</span>
</span><span id="descent-112"><a href="#descent-112"><span class="linenos">112</span></a>        <span class="c1">#NOTE: this part is a bit obsolete now, but I&#39;m still leaving it for posterity</span>
</span><span id="descent-113"><a href="#descent-113"><span class="linenos">113</span></a>        <span class="k">if</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;exact&#39;</span><span class="p">:</span>
</span><span id="descent-114"><a href="#descent-114"><span class="linenos">114</span></a>            <span class="c1"># this should update the sampler and all of its properties, including the parameters, the energy, and the psi</span>
</span><span id="descent-115"><a href="#descent-115"><span class="linenos">115</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">get_normalized_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span><span id="descent-116"><a href="#descent-116"><span class="linenos">116</span></a>        <span class="k">elif</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;tdvp&#39;</span><span class="p">:</span>
</span><span id="descent-117"><a href="#descent-117"><span class="linenos">117</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>   
</span><span id="descent-118"><a href="#descent-118"><span class="linenos">118</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="n">penalize_norm</span> <span class="c1"># do you want to penalize the norm during training?</span>
</span><span id="descent-119"><a href="#descent-119"><span class="linenos">119</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="n">Lnorm_importance</span> <span class="c1"># how important is the norm loss?</span>
</span><span id="descent-120"><a href="#descent-120"><span class="linenos">120</span></a>
</span><span id="descent-121"><a href="#descent-121"><span class="linenos">121</span></a>        <span class="c1"># regularization</span>
</span><span id="descent-122"><a href="#descent-122"><span class="linenos">122</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">regularization</span>
</span><span id="descent-123"><a href="#descent-123"><span class="linenos">123</span></a>        <span class="c1"># regulator value</span>
</span><span id="descent-124"><a href="#descent-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="descent-125"><a href="#descent-125"><span class="linenos">125</span></a>
</span><span id="descent-126"><a href="#descent-126"><span class="linenos">126</span></a>        <span class="c1"># geometric?</span>
</span><span id="descent-127"><a href="#descent-127"><span class="linenos">127</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">geo</span> <span class="o">=</span> <span class="n">geometric</span>
</span><span id="descent-128"><a href="#descent-128"><span class="linenos">128</span></a>
</span><span id="descent-129"><a href="#descent-129"><span class="linenos">129</span></a>        <span class="c1">#taming</span>
</span><span id="descent-130"><a href="#descent-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="n">refactoring</span>
</span><span id="descent-131"><a href="#descent-131"><span class="linenos">131</span></a>
</span><span id="descent-132"><a href="#descent-132"><span class="linenos">132</span></a>        <span class="c1"># use lagrange multipliers?</span>
</span><span id="descent-133"><a href="#descent-133"><span class="linenos">133</span></a>        <span class="k">if</span> <span class="n">lagrange</span><span class="p">:</span>
</span><span id="descent-134"><a href="#descent-134"><span class="linenos">134</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="descent-135"><a href="#descent-135"><span class="linenos">135</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent-136"><a href="#descent-136"><span class="linenos">136</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent-137"><a href="#descent-137"><span class="linenos">137</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>
</span><span id="descent-138"><a href="#descent-138"><span class="linenos">138</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="descent-139"><a href="#descent-139"><span class="linenos">139</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent-140"><a href="#descent-140"><span class="linenos">140</span></a>
</span><span id="descent-141"><a href="#descent-141"><span class="linenos">141</span></a>        <span class="c1"># trackable quantities</span>
</span><span id="descent-142"><a href="#descent-142"><span class="linenos">142</span></a>        <span class="c1">#self.states = [self.sampler.network.parameters.all * np.exp(1j*phase)] # list with all the states of the proccess</span>
</span><span id="descent-143"><a href="#descent-143"><span class="linenos">143</span></a>        <span class="c1">#TODO</span>
</span><span id="descent-144"><a href="#descent-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase</span><span class="p">)]</span>
</span><span id="descent-145"><a href="#descent-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">]</span> <span class="c1"># values of the loss function</span>
</span><span id="descent-146"><a href="#descent-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">psis</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span><span id="descent-147"><a href="#descent-147"><span class="linenos">147</span></a>
</span><span id="descent-148"><a href="#descent-148"><span class="linenos">148</span></a>        <span class="c1"># more trackable</span>
</span><span id="descent-149"><a href="#descent-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normF</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">)]</span>
</span><span id="descent-150"><a href="#descent-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">))]</span>
</span><span id="descent-151"><a href="#descent-151"><span class="linenos">151</span></a>
</span><span id="descent-152"><a href="#descent-152"><span class="linenos">152</span></a>    <span class="k">def</span> <span class="nf">geo_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="descent-153"><a href="#descent-153"><span class="linenos">153</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-154"><a href="#descent-154"><span class="linenos">154</span></a><span class="sd">        Calculates the gradient using the geometric implementation.</span>
</span><span id="descent-155"><a href="#descent-155"><span class="linenos">155</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-156"><a href="#descent-156"><span class="linenos">156</span></a>
</span><span id="descent-157"><a href="#descent-157"><span class="linenos">157</span></a>        <span class="c1"># Quantum Fisher Matrix and the energy gradient</span>
</span><span id="descent-158"><a href="#descent-158"><span class="linenos">158</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">))</span>
</span><span id="descent-159"><a href="#descent-159"><span class="linenos">159</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="descent-160"><a href="#descent-160"><span class="linenos">160</span></a>
</span><span id="descent-161"><a href="#descent-161"><span class="linenos">161</span></a>        <span class="c1"># split them into real parametrization</span>
</span><span id="descent-162"><a href="#descent-162"><span class="linenos">162</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">S</span><span class="p">,[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="n">j</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
</span><span id="descent-163"><a href="#descent-163"><span class="linenos">163</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">F</span><span class="p">,[</span><span class="mf">1.</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">])</span>
</span><span id="descent-164"><a href="#descent-164"><span class="linenos">164</span></a>        <span class="n">metric</span> <span class="o">=</span> <span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</span><span id="descent-165"><a href="#descent-165"><span class="linenos">165</span></a>
</span><span id="descent-166"><a href="#descent-166"><span class="linenos">166</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">rcond</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># solve the problem</span>
</span><span id="descent-167"><a href="#descent-167"><span class="linenos">167</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># get back into the complex</span>
</span><span id="descent-168"><a href="#descent-168"><span class="linenos">168</span></a>
</span><span id="descent-169"><a href="#descent-169"><span class="linenos">169</span></a>        <span class="k">return</span> <span class="n">res</span>
</span><span id="descent-170"><a href="#descent-170"><span class="linenos">170</span></a>
</span><span id="descent-171"><a href="#descent-171"><span class="linenos">171</span></a>    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="descent-172"><a href="#descent-172"><span class="linenos">172</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-173"><a href="#descent-173"><span class="linenos">173</span></a><span class="sd">        Calculates the analytical gradient of the loss function.\\</span>
</span><span id="descent-174"><a href="#descent-174"><span class="linenos">174</span></a><span class="sd">        This is obtained by the &#39;stochastic reconfiguration method&#39; - check litterature.</span>
</span><span id="descent-175"><a href="#descent-175"><span class="linenos">175</span></a>
</span><span id="descent-176"><a href="#descent-176"><span class="linenos">176</span></a><span class="sd">        UPDATE THIS:</span>
</span><span id="descent-177"><a href="#descent-177"><span class="linenos">177</span></a><span class="sd">        The gradient itself is obtained convergently, through the `np.linalg.lstsq` function (rather than regularization and pseudoinversion).</span>
</span><span id="descent-178"><a href="#descent-178"><span class="linenos">178</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-179"><a href="#descent-179"><span class="linenos">179</span></a>        <span class="c1">#return np.linalg.lstsq(self.sampler.S, -self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="descent-180"><a href="#descent-180"><span class="linenos">180</span></a>        <span class="c1">#return self.sampler.F</span>
</span><span id="descent-181"><a href="#descent-181"><span class="linenos">181</span></a>
</span><span id="descent-182"><a href="#descent-182"><span class="linenos">182</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo</span><span class="p">:</span>
</span><span id="descent-183"><a href="#descent-183"><span class="linenos">183</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="descent-184"><a href="#descent-184"><span class="linenos">184</span></a>            <span class="c1">#return np.linalg.lstsq(self.sampler.S + self.epsilon(s)*np.eye(len(self.sampler.S)), self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="descent-185"><a href="#descent-185"><span class="linenos">185</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="descent-186"><a href="#descent-186"><span class="linenos">186</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo_gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
</span><span id="descent-187"><a href="#descent-187"><span class="linenos">187</span></a>
</span><span id="descent-188"><a href="#descent-188"><span class="linenos">188</span></a>    <span class="c1">#region normalizing </span>
</span><span id="descent-189"><a href="#descent-189"><span class="linenos">189</span></a>    <span class="c1">#NOTE: this part is a bit obsolete now, but I&#39;m still leaving it for posterity</span>
</span><span id="descent-190"><a href="#descent-190"><span class="linenos">190</span></a>
</span><span id="descent-191"><a href="#descent-191"><span class="linenos">191</span></a>    <span class="k">def</span> <span class="nf">compute_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="descent-192"><a href="#descent-192"><span class="linenos">192</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loss for norm optimization: `L=(1-&lt;psi|psi&gt;)^2.` &quot;&quot;&quot;</span>
</span><span id="descent-193"><a href="#descent-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</span><span id="descent-194"><a href="#descent-194"><span class="linenos">194</span></a>    
</span><span id="descent-195"><a href="#descent-195"><span class="linenos">195</span></a>    <span class="k">def</span> <span class="nf">numerical_norm_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
</span><span id="descent-196"><a href="#descent-196"><span class="linenos">196</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-197"><a href="#descent-197"><span class="linenos">197</span></a><span class="sd">        Compute the numerical gradient of L_norm with respect to w.\\</span>
</span><span id="descent-198"><a href="#descent-198"><span class="linenos">198</span></a><span class="sd">        Uses the derivative scheme from Wirtinger calculus. See Torch documentation.</span>
</span><span id="descent-199"><a href="#descent-199"><span class="linenos">199</span></a>
</span><span id="descent-200"><a href="#descent-200"><span class="linenos">200</span></a><span class="sd">        - `epsilon`: infinitesimal step in parameters</span>
</span><span id="descent-201"><a href="#descent-201"><span class="linenos">201</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-202"><a href="#descent-202"><span class="linenos">202</span></a>        <span class="n">w</span> <span class="o">=</span> <span class="n">splice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">all</span><span class="p">)</span> <span class="c1"># save weights</span>
</span><span id="descent-203"><a href="#descent-203"><span class="linenos">203</span></a>        <span class="n">w_patched</span> <span class="o">=</span> <span class="n">patch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span id="descent-204"><a href="#descent-204"><span class="linenos">204</span></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="descent-205"><a href="#descent-205"><span class="linenos">205</span></a>        
</span><span id="descent-206"><a href="#descent-206"><span class="linenos">206</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">)):</span>
</span><span id="descent-207"><a href="#descent-207"><span class="linenos">207</span></a>            <span class="c1"># calculate positive- and negative-moved *spliced* weights</span>
</span><span id="descent-208"><a href="#descent-208"><span class="linenos">208</span></a>            <span class="c1"># not the most optimal way to do things, thank ChatGPT for that</span>
</span><span id="descent-209"><a href="#descent-209"><span class="linenos">209</span></a>            <span class="n">w_pos</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="descent-210"><a href="#descent-210"><span class="linenos">210</span></a>            <span class="n">w_neg</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="descent-211"><a href="#descent-211"><span class="linenos">211</span></a>            <span class="n">w_pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
</span><span id="descent-212"><a href="#descent-212"><span class="linenos">212</span></a>            <span class="n">w_neg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
</span><span id="descent-213"><a href="#descent-213"><span class="linenos">213</span></a>
</span><span id="descent-214"><a href="#descent-214"><span class="linenos">214</span></a>            <span class="c1"># update NN weights (*patched* ofc) and calculate the loss for both positive and negative</span>
</span><span id="descent-215"><a href="#descent-215"><span class="linenos">215</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_pos</span><span class="p">))</span>
</span><span id="descent-216"><a href="#descent-216"><span class="linenos">216</span></a>            <span class="n">loss_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="descent-217"><a href="#descent-217"><span class="linenos">217</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_neg</span><span class="p">))</span>
</span><span id="descent-218"><a href="#descent-218"><span class="linenos">218</span></a>            <span class="n">loss_neg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="descent-219"><a href="#descent-219"><span class="linenos">219</span></a>            
</span><span id="descent-220"><a href="#descent-220"><span class="linenos">220</span></a>            <span class="c1"># restore the original *patched* weights after computation</span>
</span><span id="descent-221"><a href="#descent-221"><span class="linenos">221</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w_patched</span><span class="p">)</span> 
</span><span id="descent-222"><a href="#descent-222"><span class="linenos">222</span></a>
</span><span id="descent-223"><a href="#descent-223"><span class="linenos">223</span></a>            <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_pos</span> <span class="o">-</span> <span class="n">loss_neg</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="p">)</span>  <span class="c1"># Central difference approximation</span>
</span><span id="descent-224"><a href="#descent-224"><span class="linenos">224</span></a>
</span><span id="descent-225"><a href="#descent-225"><span class="linenos">225</span></a>        <span class="k">return</span> <span class="n">patch</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">*</span><span class="mf">0.5</span>
</span><span id="descent-226"><a href="#descent-226"><span class="linenos">226</span></a>    
</span><span id="descent-227"><a href="#descent-227"><span class="linenos">227</span></a>    <span class="k">def</span> <span class="nf">normalise_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
</span><span id="descent-228"><a href="#descent-228"><span class="linenos">228</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-229"><a href="#descent-229"><span class="linenos">229</span></a><span class="sd">        Attempts to find a set of network parameters which represent a normalised wave function.\\</span>
</span><span id="descent-230"><a href="#descent-230"><span class="linenos">230</span></a><span class="sd">        This is done by optimizing the loss `L = (1-&lt;psi|psi&gt;)**2` with gradient descent.</span>
</span><span id="descent-231"><a href="#descent-231"><span class="linenos">231</span></a>
</span><span id="descent-232"><a href="#descent-232"><span class="linenos">232</span></a><span class="sd">        - `tolerance`: controls how precise the convergence should be</span>
</span><span id="descent-233"><a href="#descent-233"><span class="linenos">233</span></a><span class="sd">        - `maxiters`: manual break for the training loop if it goes into too many steps</span>
</span><span id="descent-234"><a href="#descent-234"><span class="linenos">234</span></a>
</span><span id="descent-235"><a href="#descent-235"><span class="linenos">235</span></a><span class="sd">        Fun fact: normaliSe is British, normaliZe is American English</span>
</span><span id="descent-236"><a href="#descent-236"><span class="linenos">236</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-237"><a href="#descent-237"><span class="linenos">237</span></a>        <span class="c1"># counter for manual loop break</span>
</span><span id="descent-238"><a href="#descent-238"><span class="linenos">238</span></a>        <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="descent-239"><a href="#descent-239"><span class="linenos">239</span></a>
</span><span id="descent-240"><a href="#descent-240"><span class="linenos">240</span></a>        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">:</span>
</span><span id="descent-241"><a href="#descent-241"><span class="linenos">241</span></a>            <span class="c1"># 1. calculate the gradient</span>
</span><span id="descent-242"><a href="#descent-242"><span class="linenos">242</span></a>            <span class="c1">#grad = self.numerical_norm_gradient()</span>
</span><span id="descent-243"><a href="#descent-243"><span class="linenos">243</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent-244"><a href="#descent-244"><span class="linenos">244</span></a>
</span><span id="descent-245"><a href="#descent-245"><span class="linenos">245</span></a>            <span class="c1"># 3. update weights by gradient descent</span>
</span><span id="descent-246"><a href="#descent-246"><span class="linenos">246</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span>
</span><span id="descent-247"><a href="#descent-247"><span class="linenos">247</span></a>                                <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
</span><span id="descent-248"><a href="#descent-248"><span class="linenos">248</span></a>
</span><span id="descent-249"><a href="#descent-249"><span class="linenos">249</span></a>            <span class="c1"># 4. breaking conditions</span>
</span><span id="descent-250"><a href="#descent-250"><span class="linenos">250</span></a>            <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="descent-251"><a href="#descent-251"><span class="linenos">251</span></a>            <span class="k">if</span> <span class="n">iters</span> <span class="o">&gt;=</span> <span class="n">maxiters</span><span class="p">:</span>
</span><span id="descent-252"><a href="#descent-252"><span class="linenos">252</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loop broken after &quot;</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="s2">&quot; iterations&quot;</span><span class="p">)</span>
</span><span id="descent-253"><a href="#descent-253"><span class="linenos">253</span></a>                <span class="k">break</span>
</span><span id="descent-254"><a href="#descent-254"><span class="linenos">254</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parametric normalization complete, norm = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span><span id="descent-255"><a href="#descent-255"><span class="linenos">255</span></a>        
</span><span id="descent-256"><a href="#descent-256"><span class="linenos">256</span></a>    <span class="k">def</span> <span class="nf">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradLnorm</span><span class="p">):</span>
</span><span id="descent-257"><a href="#descent-257"><span class="linenos">257</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-258"><a href="#descent-258"><span class="linenos">258</span></a><span class="sd">        Rescales the `gradLnorm` so that its norm has an upper limit corresponding to the maximum norm of the energy gradient in the whole simulation.\\</span>
</span><span id="descent-259"><a href="#descent-259"><span class="linenos">259</span></a><span class="sd">        This means that, if its norm is bigger, then reduce it to that limit, otherwise do nothing.</span>
</span><span id="descent-260"><a href="#descent-260"><span class="linenos">260</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-261"><a href="#descent-261"><span class="linenos">261</span></a>        <span class="c1">#NOTE: garbage code</span>
</span><span id="descent-262"><a href="#descent-262"><span class="linenos">262</span></a>            <span class="c1">#self.normSF.append(np.linalg.norm(self.gradient() / (1. + self.eta*np.linalg.norm(self.gradient())) ) if self.taming else np.linalg.norm(self.gradient()))</span>
</span><span id="descent-263"><a href="#descent-263"><span class="linenos">263</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad / (1. + self.eta*np.linalg.norm(self.sampler.Lnorm_grad)) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent-264"><a href="#descent-264"><span class="linenos">264</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(gradLnorm * np.linalg.norm(self.sampler.F) / np.linalg.norm(gradLnorm) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent-265"><a href="#descent-265"><span class="linenos">265</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad * max(self.normF) / np.linalg.norm(self.sampler.Lnorm_grad) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent-266"><a href="#descent-266"><span class="linenos">266</span></a>            
</span><span id="descent-267"><a href="#descent-267"><span class="linenos">267</span></a>            <span class="c1">#grad = grad / (1 + self.eta*np.linalg.norm(grad))</span>
</span><span id="descent-268"><a href="#descent-268"><span class="linenos">268</span></a>            <span class="c1">#gradLnorm = gradLnorm / (1 + self.eta*np.linalg.norm(gradLnorm))</span>
</span><span id="descent-269"><a href="#descent-269"><span class="linenos">269</span></a>            <span class="c1">#gradLnorm = gradLnorm * max(self.normF) / np.linalg.norm(gradLnorm)</span>
</span><span id="descent-270"><a href="#descent-270"><span class="linenos">270</span></a>        <span class="c1"># hopefully you skipped this</span>
</span><span id="descent-271"><a href="#descent-271"><span class="linenos">271</span></a>
</span><span id="descent-272"><a href="#descent-272"><span class="linenos">272</span></a>        <span class="c1"># upper limit to the norm</span>
</span><span id="descent-273"><a href="#descent-273"><span class="linenos">273</span></a>        <span class="n">upper_limit</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="p">)</span>
</span><span id="descent-274"><a href="#descent-274"><span class="linenos">274</span></a>        <span class="c1">#upper_limit= self.normF[-1]</span>
</span><span id="descent-275"><a href="#descent-275"><span class="linenos">275</span></a>        <span class="c1">#gradLnorm = np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S))) @ gradLnorm</span>
</span><span id="descent-276"><a href="#descent-276"><span class="linenos">276</span></a>
</span><span id="descent-277"><a href="#descent-277"><span class="linenos">277</span></a>        <span class="c1"># see if it&#39;s bigger</span>
</span><span id="descent-278"><a href="#descent-278"><span class="linenos">278</span></a>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">upper_limit</span><span class="p">:</span>
</span><span id="descent-279"><a href="#descent-279"><span class="linenos">279</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="n">upper_limit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="descent-280"><a href="#descent-280"><span class="linenos">280</span></a>
</span><span id="descent-281"><a href="#descent-281"><span class="linenos">281</span></a>        <span class="c1">#return np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S)))@gradLnorm</span>
</span><span id="descent-282"><a href="#descent-282"><span class="linenos">282</span></a>        <span class="k">return</span> <span class="n">gradLnorm</span>
</span><span id="descent-283"><a href="#descent-283"><span class="linenos">283</span></a>
</span><span id="descent-284"><a href="#descent-284"><span class="linenos">284</span></a>    <span class="c1">#endregion</span>
</span><span id="descent-285"><a href="#descent-285"><span class="linenos">285</span></a>
</span><span id="descent-286"><a href="#descent-286"><span class="linenos">286</span></a>    <span class="k">def</span> <span class="nf">lagrange_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="descent-287"><a href="#descent-287"><span class="linenos">287</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-288"><a href="#descent-288"><span class="linenos">288</span></a><span class="sd">        Performs an update of parameters that conserves the norm using Lagrange multipler(s).</span>
</span><span id="descent-289"><a href="#descent-289"><span class="linenos">289</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-290"><a href="#descent-290"><span class="linenos">290</span></a>        <span class="c1"># LHS of the EoM</span>
</span><span id="descent-291"><a href="#descent-291"><span class="linenos">291</span></a>        <span class="c1"># make the matrix</span>
</span><span id="descent-292"><a href="#descent-292"><span class="linenos">292</span></a>        <span class="c1"># apply the regularization only on the S matrix?</span>
</span><span id="descent-293"><a href="#descent-293"><span class="linenos">293</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)),</span> 
</span><span id="descent-294"><a href="#descent-294"><span class="linenos">294</span></a>                             <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent-295"><a href="#descent-295"><span class="linenos">295</span></a>                             <span class="p">))</span>
</span><span id="descent-296"><a href="#descent-296"><span class="linenos">296</span></a>        <span class="c1"># M = np.column_stack((self.sampler.S, </span>
</span><span id="descent-297"><a href="#descent-297"><span class="linenos">297</span></a>        <span class="c1">#                      self.sampler.Lnorm_grad))</span>
</span><span id="descent-298"><a href="#descent-298"><span class="linenos">298</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="descent-299"><a href="#descent-299"><span class="linenos">299</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">,</span> <span class="mf">0.</span>
</span><span id="descent-300"><a href="#descent-300"><span class="linenos">300</span></a>            <span class="p">)))</span>
</span><span id="descent-301"><a href="#descent-301"><span class="linenos">301</span></a>
</span><span id="descent-302"><a href="#descent-302"><span class="linenos">302</span></a>        <span class="c1"># or regularization of the WHOLE matrix?</span>
</span><span id="descent-303"><a href="#descent-303"><span class="linenos">303</span></a>        <span class="c1"># M = M + self.epsilon(s)*np.eye(len(M))</span>
</span><span id="descent-304"><a href="#descent-304"><span class="linenos">304</span></a>        
</span><span id="descent-305"><a href="#descent-305"><span class="linenos">305</span></a>        <span class="c1"># RHS of the EoM</span>
</span><span id="descent-306"><a href="#descent-306"><span class="linenos">306</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
</span><span id="descent-307"><a href="#descent-307"><span class="linenos">307</span></a>
</span><span id="descent-308"><a href="#descent-308"><span class="linenos">308</span></a>        <span class="c1"># solution</span>
</span><span id="descent-309"><a href="#descent-309"><span class="linenos">309</span></a>        <span class="c1"># don&#39;t forget to leave out the last element because that&#39;s the multipler</span>
</span><span id="descent-310"><a href="#descent-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">@</span> <span class="n">F</span><span class="p">)</span> <span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="descent-311"><a href="#descent-311"><span class="linenos">311</span></a>
</span><span id="descent-312"><a href="#descent-312"><span class="linenos">312</span></a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">precision</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">miniter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="descent-313"><a href="#descent-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-314"><a href="#descent-314"><span class="linenos">314</span></a><span class="sd">        Performs the gradient descent algorithm.</span>
</span><span id="descent-315"><a href="#descent-315"><span class="linenos">315</span></a>
</span><span id="descent-316"><a href="#descent-316"><span class="linenos">316</span></a><span class="sd">        - `precision`: convergence criterion for the loss function</span>
</span><span id="descent-317"><a href="#descent-317"><span class="linenos">317</span></a><span class="sd">        - `miniter`: minimum number of iterations</span>
</span><span id="descent-318"><a href="#descent-318"><span class="linenos">318</span></a><span class="sd">        - `maxiter`: maximum number of iterations</span>
</span><span id="descent-319"><a href="#descent-319"><span class="linenos">319</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-320"><a href="#descent-320"><span class="linenos">320</span></a>        <span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># iterations</span>
</span><span id="descent-321"><a href="#descent-321"><span class="linenos">321</span></a>        <span class="c1">#params = np.real(self.states[0].copy()) + 1j*np.zeros(len(self.states[0])) # initial parameters</span>
</span><span id="descent-322"><a href="#descent-322"><span class="linenos">322</span></a>        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
</span><span id="descent-323"><a href="#descent-323"><span class="linenos">323</span></a>
</span><span id="descent-324"><a href="#descent-324"><span class="linenos">324</span></a>        <span class="c1"># main loop</span>
</span><span id="descent-325"><a href="#descent-325"><span class="linenos">325</span></a>        <span class="k">while</span> <span class="nb">iter</span> <span class="o">&lt;=</span> <span class="n">maxiter</span><span class="p">:</span>
</span><span id="descent-326"><a href="#descent-326"><span class="linenos">326</span></a>            <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="descent-327"><a href="#descent-327"><span class="linenos">327</span></a>
</span><span id="descent-328"><a href="#descent-328"><span class="linenos">328</span></a>            <span class="c1"># if self.zigzag:</span>
</span><span id="descent-329"><a href="#descent-329"><span class="linenos">329</span></a>            <span class="c1">#     self.normalise_params()</span>
</span><span id="descent-330"><a href="#descent-330"><span class="linenos">330</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="descent-331"><a href="#descent-331"><span class="linenos">331</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="nb">iter</span><span class="p">)</span> <span class="c1"># gradient</span>
</span><span id="descent-332"><a href="#descent-332"><span class="linenos">332</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent-333"><a href="#descent-333"><span class="linenos">333</span></a>
</span><span id="descent-334"><a href="#descent-334"><span class="linenos">334</span></a>            <span class="c1">#do you refactor?</span>
</span><span id="descent-335"><a href="#descent-335"><span class="linenos">335</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span><span class="p">:</span>
</span><span id="descent-336"><a href="#descent-336"><span class="linenos">336</span></a>                <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="descent-337"><a href="#descent-337"><span class="linenos">337</span></a>            <span class="c1">#do you penalize?</span>
</span><span id="descent-338"><a href="#descent-338"><span class="linenos">338</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span><span class="p">:</span> <span class="c1"># if you&#39;re also normalizing, penalize the norm in the gradient</span>
</span><span id="descent-339"><a href="#descent-339"><span class="linenos">339</span></a>                <span class="n">grad</span> <span class="o">+=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="descent-340"><a href="#descent-340"><span class="linenos">340</span></a>            <span class="c1"># if not either of those things, you are probably lagranging</span>
</span><span id="descent-341"><a href="#descent-341"><span class="linenos">341</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span><span class="p">:</span>
</span><span id="descent-342"><a href="#descent-342"><span class="linenos">342</span></a>                <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange_gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">)</span>
</span><span id="descent-343"><a href="#descent-343"><span class="linenos">343</span></a>
</span><span id="descent-344"><a href="#descent-344"><span class="linenos">344</span></a>            <span class="n">params</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># descent</span>
</span><span id="descent-345"><a href="#descent-345"><span class="linenos">345</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="c1"># change all the elements of the sampler (network, S, F, etc...)</span>
</span><span id="descent-346"><a href="#descent-346"><span class="linenos">346</span></a>
</span><span id="descent-347"><a href="#descent-347"><span class="linenos">347</span></a>            <span class="c1"># accumulate things</span>
</span><span id="descent-348"><a href="#descent-348"><span class="linenos">348</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span> <span class="c1"># append the loss function</span>
</span><span id="descent-349"><a href="#descent-349"><span class="linenos">349</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="descent-350"><a href="#descent-350"><span class="linenos">350</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">psis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="descent-351"><a href="#descent-351"><span class="linenos">351</span></a>            <span class="c1">#self.normF.append(np.linalg.norm(self.sampler.F))</span>
</span><span id="descent-352"><a href="#descent-352"><span class="linenos">352</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
</span><span id="descent-353"><a href="#descent-353"><span class="linenos">353</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">)))</span>
</span><span id="descent-354"><a href="#descent-354"><span class="linenos">354</span></a>            
</span><span id="descent-355"><a href="#descent-355"><span class="linenos">355</span></a>            <span class="c1"># sponsored messages</span>
</span><span id="descent-356"><a href="#descent-356"><span class="linenos">356</span></a>            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">iter</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="descent-357"><a href="#descent-357"><span class="linenos">357</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="s2">&quot;... energy: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="descent-358"><a href="#descent-358"><span class="linenos">358</span></a>            
</span><span id="descent-359"><a href="#descent-359"><span class="linenos">359</span></a>            <span class="c1"># check criteria</span>
</span><span id="descent-360"><a href="#descent-360"><span class="linenos">360</span></a>            <span class="k">if</span> <span class="nb">iter</span> <span class="o">&gt;</span> <span class="n">miniter</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">precision</span><span class="p">:</span>
</span><span id="descent-361"><a href="#descent-361"><span class="linenos">361</span></a>                <span class="k">break</span>
</span><span id="descent-362"><a href="#descent-362"><span class="linenos">362</span></a>        
</span><span id="descent-363"><a href="#descent-363"><span class="linenos">363</span></a>    
</span><span id="descent-364"><a href="#descent-364"><span class="linenos">364</span></a>    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">links</span><span class="p">):</span>
</span><span id="descent-365"><a href="#descent-365"><span class="linenos">365</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-366"><a href="#descent-366"><span class="linenos">366</span></a><span class="sd">        Calculates the correlation functions at each state of the process.\\</span>
</span><span id="descent-367"><a href="#descent-367"><span class="linenos">367</span></a><span class="sd">        Correlatoins are written in a `link -&gt; list` dictionary, where the `list` is all the values of the correlation at the lattice `link` during the process.</span>
</span><span id="descent-368"><a href="#descent-368"><span class="linenos">368</span></a>
</span><span id="descent-369"><a href="#descent-369"><span class="linenos">369</span></a><span class="sd">        - `links`: an array of tuples representing links on the lattice where the calculation is performed </span>
</span><span id="descent-370"><a href="#descent-370"><span class="linenos">370</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-371"><a href="#descent-371"><span class="linenos">371</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="descent-372"><a href="#descent-372"><span class="linenos">372</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="descent-373"><a href="#descent-373"><span class="linenos">373</span></a>
</span><span id="descent-374"><a href="#descent-374"><span class="linenos">374</span></a>        <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
</span><span id="descent-375"><a href="#descent-375"><span class="linenos">375</span></a>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculating correlation at link&quot;</span><span class="p">,</span> <span class="n">link</span><span class="p">)</span>
</span><span id="descent-376"><a href="#descent-376"><span class="linenos">376</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="descent-377"><a href="#descent-377"><span class="linenos">377</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="descent-378"><a href="#descent-378"><span class="linenos">378</span></a>
</span><span id="descent-379"><a href="#descent-379"><span class="linenos">379</span></a>            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">))</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
</span><span id="descent-380"><a href="#descent-380"><span class="linenos">380</span></a>                <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
</span><span id="descent-381"><a href="#descent-381"><span class="linenos">381</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</span><span id="descent-382"><a href="#descent-382"><span class="linenos">382</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Cvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent-383"><a href="#descent-383"><span class="linenos">383</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Czvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent-384"><a href="#descent-384"><span class="linenos">384</span></a>                    
</span><span id="descent-385"><a href="#descent-385"><span class="linenos">385</span></a>                    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="descent-386"><a href="#descent-386"><span class="linenos">386</span></a>    
</span><span id="descent-387"><a href="#descent-387"><span class="linenos">387</span></a>
</span><span id="descent-388"><a href="#descent-388"><span class="linenos">388</span></a>    <span class="k">def</span> <span class="nf">nudge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase_nudge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">):</span>
</span><span id="descent-389"><a href="#descent-389"><span class="linenos">389</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent-390"><a href="#descent-390"><span class="linenos">390</span></a><span class="sd">        Corrects the weights of the neural network to be complex, but still have the same energy.\\</span>
</span><span id="descent-391"><a href="#descent-391"><span class="linenos">391</span></a><span class="sd">        Uses a root-finding algorithm to find the state whose energy offset is zero. This is equivalent to supervised learning.</span>
</span><span id="descent-392"><a href="#descent-392"><span class="linenos">392</span></a>
</span><span id="descent-393"><a href="#descent-393"><span class="linenos">393</span></a><span class="sd">        1. Initialize weights: take the trained weights in `states[-1]` and multiply them with `exp(1j*phase_nudge)`.</span>
</span><span id="descent-394"><a href="#descent-394"><span class="linenos">394</span></a><span class="sd">        2. Find the root: construct a function `L (w) = |E_0 - E(w)|`, where `E_0` is the pre-trained energy, and find the weights `w` for which this function has a root.</span>
</span><span id="descent-395"><a href="#descent-395"><span class="linenos">395</span></a>
</span><span id="descent-396"><a href="#descent-396"><span class="linenos">396</span></a><span class="sd">        USE only if you have performed the graident descent by the `descent()` function. Otherwise the results are senseless.</span>
</span><span id="descent-397"><a href="#descent-397"><span class="linenos">397</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent-398"><a href="#descent-398"><span class="linenos">398</span></a>
</span><span id="descent-399"><a href="#descent-399"><span class="linenos">399</span></a>        <span class="c1"># 1. nudge the parameters</span>
</span><span id="descent-400"><a href="#descent-400"><span class="linenos">400</span></a>        <span class="n">w0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase_nudge</span><span class="p">)</span>
</span><span id="descent-401"><a href="#descent-401"><span class="linenos">401</span></a>
</span><span id="descent-402"><a href="#descent-402"><span class="linenos">402</span></a>        <span class="c1"># 2. create the function</span>
</span><span id="descent-403"><a href="#descent-403"><span class="linenos">403</span></a>        <span class="k">def</span> <span class="nf">nudgeloss</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="descent-404"><a href="#descent-404"><span class="linenos">404</span></a>            <span class="c1"># update the sampler</span>
</span><span id="descent-405"><a href="#descent-405"><span class="linenos">405</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">just_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="descent-406"><a href="#descent-406"><span class="linenos">406</span></a>
</span><span id="descent-407"><a href="#descent-407"><span class="linenos">407</span></a>            <span class="c1"># return the value</span>
</span><span id="descent-408"><a href="#descent-408"><span class="linenos">408</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent-409"><a href="#descent-409"><span class="linenos">409</span></a>        
</span><span id="descent-410"><a href="#descent-410"><span class="linenos">410</span></a>        <span class="c1"># 3. run the root finder</span>
</span><span id="descent-411"><a href="#descent-411"><span class="linenos">411</span></a>        <span class="n">nudged_weights</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">newton</span><span class="p">(</span><span class="n">nudgeloss</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)</span>
</span><span id="descent-412"><a href="#descent-412"><span class="linenos">412</span></a>
</span><span id="descent-413"><a href="#descent-413"><span class="linenos">413</span></a>        <span class="k">return</span> <span class="n">nudged_weights</span>
</span></pre></div>


            <div class="docstring"><p>A gradient descent into the ground state of the lattice Heisenberg model.</p>

<h1 id="variables">Variables</h1>

<ul>
<li><code>lattice</code>: details about the spin lattice</li>
<li><code>alpha</code>: network density</li>
<li><code><a href="#descent.eta">eta</a></code>: learning rate</li>
<li><code>tinv</code>: is the network translationally invariant? this changes the number of used parameters</li>
<li><code>afm_sector</code>: keep only the zero-magnetization sector of the Hiblert space?</li>
<li><code>scale</code>: multiply the wave function with this number</li>
<li><code>taming</code>: do you want to rescale the gradient(s)? this helps if they are too huge</li>
<li><code>real_parameters</code>: initialize parameters as real?</li>
<li><code><a href="#descent.epsilon">epsilon</a></code><strong>*</strong>: a function depending on the step that modifies the regularization during training</li>
<li><code>normalize_initial</code>: how do you want to recalculate weights to normalize the wave fucntion? choose: <code>'analytical'</code>, <code>'numerical'</code>, or leave default <code>None</code>.</li>
<li><code><a href="#descent.penalize_norm">penalize_norm</a></code>: do you want to penalize the norm during training?</li>
<li><code><a href="#descent.Lnorm_importance">Lnorm_importance</a></code><strong>*</strong>: a function that modifies the learning rate of normalization during training</li>
<li><code>lagrange</code>: decides whether to use lagrange multipliers to conserve the norm during training. <strong>NB</strong>: setting to <code>True</code> normalizes the initial parameters with <code>'tdvp'</code>, and overwrites <code><a href="#descent.refactoring">refactoring</a></code> and <code><a href="#descent.penalize_norm">penalize_norm</a></code> to <code>False</code>.</li>
</ul>

<p><strong>*Both of these must be functions. If you don't know what they mean, leave them as defaults.</strong></p>

<h1 id="properties">Properties</h1>

<ul>
<li><code><a href="#descent.sampler">sampler</a></code>: everything regarding the full sampling</li>
<li><code><a href="#descent.states">states</a></code>: list of network parameter states</li>
<li><code><a href="#descent.loss">loss</a></code>: list of loss function values (variational energy)</li>
<li><code><a href="#descent.psis">psis</a></code>: list of wave function vectors</li>
<li><code><a href="#descent.normF">normF</a></code>, <code>normSF</code>, <code><a href="#descent.normLnorm">normLnorm</a></code>: list of norms of gradients of F, S^-1F, L_norm respectively</li>
</ul>

<h1 id="methods">Methods</h1>

<ul>
<li><code><a href="#descent.gradient">gradient</a></code>: calculates gradient with stochastic reconfiguration</li>
<li><code><a href="#descent.optimize">optimize</a></code>: performs the descent</li>
<li><code>process (links)</code>: calculates the optimization curve for the (z)correlation function(s) at the given <code>links</code> list</li>
</ul>

<p>Regularizaiton recipe taken from the Carleo &amp; Troyer paper.</p>
</div>


                            <div id="descent.__init__" class="classattr">
                                        <input id="descent.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">descent</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">lattice</span><span class="p">:</span> <span class="n"><a href="hilbert.html#lattice">src.hilbert.lattice</a></span>,</span><span class="param">	<span class="n">alpha</span>,</span><span class="param">	<span class="n">eta</span>,</span><span class="param">	<span class="n">regularization</span><span class="o">=&lt;</span><span class="n">function</span> <span class="n">descent</span><span class="o">.&lt;</span><span class="k">lambda</span><span class="o">&gt;&gt;</span>,</span><span class="param">	<span class="n">phase</span><span class="o">=</span><span class="mf">0.0</span>,</span><span class="param">	<span class="n">tinv</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">normal_sampling</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">geometric</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">afm_sector</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">model</span><span class="o">=</span><span class="s1">&#39;heisenberg&#39;</span>,</span><span class="param">	<span class="n">gauge</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">real_parameters</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span>,</span><span class="param">	<span class="n">refactoring</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">normalize_initial</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">penalize_norm</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">Lnorm_importance</span><span class="o">=&lt;</span><span class="n">function</span> <span class="n">descent</span><span class="o">.&lt;</span><span class="k">lambda</span><span class="o">&gt;&gt;</span>,</span><span class="param">	<span class="n">lagrange</span><span class="o">=</span><span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="descent.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.__init__-89"><a href="#descent.__init__-89"><span class="linenos"> 89</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lattice</span><span class="p">:</span><span class="n">hilbert</span><span class="o">.</span><span class="n">lattice</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span>  
</span><span id="descent.__init__-90"><a href="#descent.__init__-90"><span class="linenos"> 90</span></a>                 <span class="n">regularization</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="mf">0.99</span><span class="o">**</span><span class="n">s</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">),</span> <span class="c1"># regulator?</span>
</span><span id="descent.__init__-91"><a href="#descent.__init__-91"><span class="linenos"> 91</span></a>                 <span class="n">phase</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
</span><span id="descent.__init__-92"><a href="#descent.__init__-92"><span class="linenos"> 92</span></a>                 <span class="n">tinv</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">normal_sampling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># invariance and sampling details </span>
</span><span id="descent.__init__-93"><a href="#descent.__init__-93"><span class="linenos"> 93</span></a>                 <span class="n">geometric</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># use the geometric method that i coded just for fun?</span>
</span><span id="descent.__init__-94"><a href="#descent.__init__-94"><span class="linenos"> 94</span></a>                 <span class="n">afm_sector</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># restrict to antiferromagnetic sector?</span>
</span><span id="descent.__init__-95"><a href="#descent.__init__-95"><span class="linenos"> 95</span></a>                 <span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;heisenberg&#39;</span><span class="p">,</span> <span class="c1"># which model?</span>
</span><span id="descent.__init__-96"><a href="#descent.__init__-96"><span class="linenos"> 96</span></a>                 <span class="n">gauge</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent.__init__-97"><a href="#descent.__init__-97"><span class="linenos"> 97</span></a>                 <span class="n">real_parameters</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent.__init__-98"><a href="#descent.__init__-98"><span class="linenos"> 98</span></a>                 <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="descent.__init__-99"><a href="#descent.__init__-99"><span class="linenos"> 99</span></a>                 <span class="n">refactoring</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="descent.__init__-100"><a href="#descent.__init__-100"><span class="linenos">100</span></a>                 <span class="n">normalize_initial</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="descent.__init__-101"><a href="#descent.__init__-101"><span class="linenos">101</span></a>                 <span class="n">penalize_norm</span> <span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="descent.__init__-102"><a href="#descent.__init__-102"><span class="linenos">102</span></a>                 <span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.</span><span class="p">,</span>
</span><span id="descent.__init__-103"><a href="#descent.__init__-103"><span class="linenos">103</span></a>                 <span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent.__init__-104"><a href="#descent.__init__-104"><span class="linenos">104</span></a>                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="descent.__init__-105"><a href="#descent.__init__-105"><span class="linenos">105</span></a>        
</span><span id="descent.__init__-106"><a href="#descent.__init__-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span> <span class="c1"># learning rate</span>
</span><span id="descent.__init__-107"><a href="#descent.__init__-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">full</span><span class="o">.</span><span class="n">sampler</span><span class="p">(</span><span class="n">hilbert</span><span class="o">.</span><span class="n">space</span><span class="p">(</span><span class="n">lattice</span><span class="p">,</span> <span class="n">afm</span><span class="o">=</span><span class="n">afm_sector</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">gauge</span><span class="o">=</span><span class="n">gauge</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> 
</span><span id="descent.__init__-108"><a href="#descent.__init__-108"><span class="linenos">108</span></a>                                    <span class="n">tinvariant</span><span class="o">=</span><span class="n">tinv</span><span class="p">,</span> <span class="n">normal</span><span class="o">=</span><span class="n">normal_sampling</span><span class="p">,</span> <span class="n">real</span><span class="o">=</span><span class="n">real_parameters</span><span class="p">,</span>
</span><span id="descent.__init__-109"><a href="#descent.__init__-109"><span class="linenos">109</span></a>                                    <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span> <span class="c1"># sampler containig the entire Hilbert space and the network</span>
</span><span id="descent.__init__-110"><a href="#descent.__init__-110"><span class="linenos">110</span></a>
</span><span id="descent.__init__-111"><a href="#descent.__init__-111"><span class="linenos">111</span></a>        <span class="c1"># get normalized funciton weights if you want to normalize initial state</span>
</span><span id="descent.__init__-112"><a href="#descent.__init__-112"><span class="linenos">112</span></a>        <span class="c1">#NOTE: this part is a bit obsolete now, but I&#39;m still leaving it for posterity</span>
</span><span id="descent.__init__-113"><a href="#descent.__init__-113"><span class="linenos">113</span></a>        <span class="k">if</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;exact&#39;</span><span class="p">:</span>
</span><span id="descent.__init__-114"><a href="#descent.__init__-114"><span class="linenos">114</span></a>            <span class="c1"># this should update the sampler and all of its properties, including the parameters, the energy, and the psi</span>
</span><span id="descent.__init__-115"><a href="#descent.__init__-115"><span class="linenos">115</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">get_normalized_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span><span id="descent.__init__-116"><a href="#descent.__init__-116"><span class="linenos">116</span></a>        <span class="k">elif</span> <span class="n">normalize_initial</span> <span class="o">==</span> <span class="s1">&#39;tdvp&#39;</span><span class="p">:</span>
</span><span id="descent.__init__-117"><a href="#descent.__init__-117"><span class="linenos">117</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>   
</span><span id="descent.__init__-118"><a href="#descent.__init__-118"><span class="linenos">118</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="n">penalize_norm</span> <span class="c1"># do you want to penalize the norm during training?</span>
</span><span id="descent.__init__-119"><a href="#descent.__init__-119"><span class="linenos">119</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span> <span class="o">=</span> <span class="n">Lnorm_importance</span> <span class="c1"># how important is the norm loss?</span>
</span><span id="descent.__init__-120"><a href="#descent.__init__-120"><span class="linenos">120</span></a>
</span><span id="descent.__init__-121"><a href="#descent.__init__-121"><span class="linenos">121</span></a>        <span class="c1"># regularization</span>
</span><span id="descent.__init__-122"><a href="#descent.__init__-122"><span class="linenos">122</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">regularization</span>
</span><span id="descent.__init__-123"><a href="#descent.__init__-123"><span class="linenos">123</span></a>        <span class="c1"># regulator value</span>
</span><span id="descent.__init__-124"><a href="#descent.__init__-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="descent.__init__-125"><a href="#descent.__init__-125"><span class="linenos">125</span></a>
</span><span id="descent.__init__-126"><a href="#descent.__init__-126"><span class="linenos">126</span></a>        <span class="c1"># geometric?</span>
</span><span id="descent.__init__-127"><a href="#descent.__init__-127"><span class="linenos">127</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">geo</span> <span class="o">=</span> <span class="n">geometric</span>
</span><span id="descent.__init__-128"><a href="#descent.__init__-128"><span class="linenos">128</span></a>
</span><span id="descent.__init__-129"><a href="#descent.__init__-129"><span class="linenos">129</span></a>        <span class="c1">#taming</span>
</span><span id="descent.__init__-130"><a href="#descent.__init__-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="n">refactoring</span>
</span><span id="descent.__init__-131"><a href="#descent.__init__-131"><span class="linenos">131</span></a>
</span><span id="descent.__init__-132"><a href="#descent.__init__-132"><span class="linenos">132</span></a>        <span class="c1"># use lagrange multipliers?</span>
</span><span id="descent.__init__-133"><a href="#descent.__init__-133"><span class="linenos">133</span></a>        <span class="k">if</span> <span class="n">lagrange</span><span class="p">:</span>
</span><span id="descent.__init__-134"><a href="#descent.__init__-134"><span class="linenos">134</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="descent.__init__-135"><a href="#descent.__init__-135"><span class="linenos">135</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent.__init__-136"><a href="#descent.__init__-136"><span class="linenos">136</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent.__init__-137"><a href="#descent.__init__-137"><span class="linenos">137</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normalise_params</span><span class="p">()</span>
</span><span id="descent.__init__-138"><a href="#descent.__init__-138"><span class="linenos">138</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="descent.__init__-139"><a href="#descent.__init__-139"><span class="linenos">139</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="descent.__init__-140"><a href="#descent.__init__-140"><span class="linenos">140</span></a>
</span><span id="descent.__init__-141"><a href="#descent.__init__-141"><span class="linenos">141</span></a>        <span class="c1"># trackable quantities</span>
</span><span id="descent.__init__-142"><a href="#descent.__init__-142"><span class="linenos">142</span></a>        <span class="c1">#self.states = [self.sampler.network.parameters.all * np.exp(1j*phase)] # list with all the states of the proccess</span>
</span><span id="descent.__init__-143"><a href="#descent.__init__-143"><span class="linenos">143</span></a>        <span class="c1">#TODO</span>
</span><span id="descent.__init__-144"><a href="#descent.__init__-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase</span><span class="p">)]</span>
</span><span id="descent.__init__-145"><a href="#descent.__init__-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">]</span> <span class="c1"># values of the loss function</span>
</span><span id="descent.__init__-146"><a href="#descent.__init__-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">psis</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span><span id="descent.__init__-147"><a href="#descent.__init__-147"><span class="linenos">147</span></a>
</span><span id="descent.__init__-148"><a href="#descent.__init__-148"><span class="linenos">148</span></a>        <span class="c1"># more trackable</span>
</span><span id="descent.__init__-149"><a href="#descent.__init__-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normF</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">)]</span>
</span><span id="descent.__init__-150"><a href="#descent.__init__-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">))]</span>
</span></pre></div>


    

                            </div>
                            <div id="descent.eta" class="classattr">
                                <div class="attr variable">
            <span class="name">eta</span>

        
    </div>
    <a class="headerlink" href="#descent.eta"></a>
    
    

                            </div>
                            <div id="descent.sampler" class="classattr">
                                <div class="attr variable">
            <span class="name">sampler</span>

        
    </div>
    <a class="headerlink" href="#descent.sampler"></a>
    
    

                            </div>
                            <div id="descent.penalize_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">penalize_norm</span>

        
    </div>
    <a class="headerlink" href="#descent.penalize_norm"></a>
    
    

                            </div>
                            <div id="descent.Lnorm_importance" class="classattr">
                                <div class="attr variable">
            <span class="name">Lnorm_importance</span>

        
    </div>
    <a class="headerlink" href="#descent.Lnorm_importance"></a>
    
    

                            </div>
                            <div id="descent.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span>

        
    </div>
    <a class="headerlink" href="#descent.epsilon"></a>
    
    

                            </div>
                            <div id="descent.geo" class="classattr">
                                <div class="attr variable">
            <span class="name">geo</span>

        
    </div>
    <a class="headerlink" href="#descent.geo"></a>
    
    

                            </div>
                            <div id="descent.refactoring" class="classattr">
                                <div class="attr variable">
            <span class="name">refactoring</span>

        
    </div>
    <a class="headerlink" href="#descent.refactoring"></a>
    
    

                            </div>
                            <div id="descent.states" class="classattr">
                                <div class="attr variable">
            <span class="name">states</span>

        
    </div>
    <a class="headerlink" href="#descent.states"></a>
    
    

                            </div>
                            <div id="descent.loss" class="classattr">
                                <div class="attr variable">
            <span class="name">loss</span>

        
    </div>
    <a class="headerlink" href="#descent.loss"></a>
    
    

                            </div>
                            <div id="descent.psis" class="classattr">
                                <div class="attr variable">
            <span class="name">psis</span>

        
    </div>
    <a class="headerlink" href="#descent.psis"></a>
    
    

                            </div>
                            <div id="descent.normF" class="classattr">
                                <div class="attr variable">
            <span class="name">normF</span>

        
    </div>
    <a class="headerlink" href="#descent.normF"></a>
    
    

                            </div>
                            <div id="descent.normLnorm" class="classattr">
                                <div class="attr variable">
            <span class="name">normLnorm</span>

        
    </div>
    <a class="headerlink" href="#descent.normLnorm"></a>
    
    

                            </div>
                            <div id="descent.geo_gradient" class="classattr">
                                        <input id="descent.geo_gradient-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">geo_gradient</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">s</span><span class="o">=</span><span class="mi">0</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.geo_gradient-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.geo_gradient"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.geo_gradient-152"><a href="#descent.geo_gradient-152"><span class="linenos">152</span></a>    <span class="k">def</span> <span class="nf">geo_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="descent.geo_gradient-153"><a href="#descent.geo_gradient-153"><span class="linenos">153</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.geo_gradient-154"><a href="#descent.geo_gradient-154"><span class="linenos">154</span></a><span class="sd">        Calculates the gradient using the geometric implementation.</span>
</span><span id="descent.geo_gradient-155"><a href="#descent.geo_gradient-155"><span class="linenos">155</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.geo_gradient-156"><a href="#descent.geo_gradient-156"><span class="linenos">156</span></a>
</span><span id="descent.geo_gradient-157"><a href="#descent.geo_gradient-157"><span class="linenos">157</span></a>        <span class="c1"># Quantum Fisher Matrix and the energy gradient</span>
</span><span id="descent.geo_gradient-158"><a href="#descent.geo_gradient-158"><span class="linenos">158</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">))</span>
</span><span id="descent.geo_gradient-159"><a href="#descent.geo_gradient-159"><span class="linenos">159</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="descent.geo_gradient-160"><a href="#descent.geo_gradient-160"><span class="linenos">160</span></a>
</span><span id="descent.geo_gradient-161"><a href="#descent.geo_gradient-161"><span class="linenos">161</span></a>        <span class="c1"># split them into real parametrization</span>
</span><span id="descent.geo_gradient-162"><a href="#descent.geo_gradient-162"><span class="linenos">162</span></a>        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">S</span><span class="p">,[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="n">j</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
</span><span id="descent.geo_gradient-163"><a href="#descent.geo_gradient-163"><span class="linenos">163</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">F</span><span class="p">,[</span><span class="mf">1.</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">])</span>
</span><span id="descent.geo_gradient-164"><a href="#descent.geo_gradient-164"><span class="linenos">164</span></a>        <span class="n">metric</span> <span class="o">=</span> <span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</span><span id="descent.geo_gradient-165"><a href="#descent.geo_gradient-165"><span class="linenos">165</span></a>
</span><span id="descent.geo_gradient-166"><a href="#descent.geo_gradient-166"><span class="linenos">166</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">rcond</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># solve the problem</span>
</span><span id="descent.geo_gradient-167"><a href="#descent.geo_gradient-167"><span class="linenos">167</span></a>        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># get back into the complex</span>
</span><span id="descent.geo_gradient-168"><a href="#descent.geo_gradient-168"><span class="linenos">168</span></a>
</span><span id="descent.geo_gradient-169"><a href="#descent.geo_gradient-169"><span class="linenos">169</span></a>        <span class="k">return</span> <span class="n">res</span>
</span></pre></div>


            <div class="docstring"><p>Calculates the gradient using the geometric implementation.</p>
</div>


                            </div>
                            <div id="descent.gradient" class="classattr">
                                        <input id="descent.gradient-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">gradient</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">s</span><span class="o">=</span><span class="mi">0</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.gradient-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.gradient"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.gradient-171"><a href="#descent.gradient-171"><span class="linenos">171</span></a>    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span id="descent.gradient-172"><a href="#descent.gradient-172"><span class="linenos">172</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.gradient-173"><a href="#descent.gradient-173"><span class="linenos">173</span></a><span class="sd">        Calculates the analytical gradient of the loss function.\\</span>
</span><span id="descent.gradient-174"><a href="#descent.gradient-174"><span class="linenos">174</span></a><span class="sd">        This is obtained by the &#39;stochastic reconfiguration method&#39; - check litterature.</span>
</span><span id="descent.gradient-175"><a href="#descent.gradient-175"><span class="linenos">175</span></a>
</span><span id="descent.gradient-176"><a href="#descent.gradient-176"><span class="linenos">176</span></a><span class="sd">        UPDATE THIS:</span>
</span><span id="descent.gradient-177"><a href="#descent.gradient-177"><span class="linenos">177</span></a><span class="sd">        The gradient itself is obtained convergently, through the `np.linalg.lstsq` function (rather than regularization and pseudoinversion).</span>
</span><span id="descent.gradient-178"><a href="#descent.gradient-178"><span class="linenos">178</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.gradient-179"><a href="#descent.gradient-179"><span class="linenos">179</span></a>        <span class="c1">#return np.linalg.lstsq(self.sampler.S, -self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="descent.gradient-180"><a href="#descent.gradient-180"><span class="linenos">180</span></a>        <span class="c1">#return self.sampler.F</span>
</span><span id="descent.gradient-181"><a href="#descent.gradient-181"><span class="linenos">181</span></a>
</span><span id="descent.gradient-182"><a href="#descent.gradient-182"><span class="linenos">182</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo</span><span class="p">:</span>
</span><span id="descent.gradient-183"><a href="#descent.gradient-183"><span class="linenos">183</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)))</span><span class="nd">@self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span>
</span><span id="descent.gradient-184"><a href="#descent.gradient-184"><span class="linenos">184</span></a>            <span class="c1">#return np.linalg.lstsq(self.sampler.S + self.epsilon(s)*np.eye(len(self.sampler.S)), self.sampler.F, rcond = 1e-10)[0]</span>
</span><span id="descent.gradient-185"><a href="#descent.gradient-185"><span class="linenos">185</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="descent.gradient-186"><a href="#descent.gradient-186"><span class="linenos">186</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">geo_gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Calculates the analytical gradient of the loss function.\
This is obtained by the 'stochastic reconfiguration method' - check litterature.</p>

<p>UPDATE THIS:
The gradient itself is obtained convergently, through the <code>np.linalg.lstsq</code> function (rather than regularization and pseudoinversion).</p>
</div>


                            </div>
                            <div id="descent.compute_Lnorm" class="classattr">
                                        <input id="descent.compute_Lnorm-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">compute_Lnorm</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.compute_Lnorm-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.compute_Lnorm"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.compute_Lnorm-191"><a href="#descent.compute_Lnorm-191"><span class="linenos">191</span></a>    <span class="k">def</span> <span class="nf">compute_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="descent.compute_Lnorm-192"><a href="#descent.compute_Lnorm-192"><span class="linenos">192</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loss for norm optimization: `L=(1-&lt;psi|psi&gt;)^2.` &quot;&quot;&quot;</span>
</span><span id="descent.compute_Lnorm-193"><a href="#descent.compute_Lnorm-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</span></pre></div>


            <div class="docstring"><p>Loss for norm optimization: <code>L=(1-&lt;psi|psi&gt;)^2.</code></p>
</div>


                            </div>
                            <div id="descent.numerical_norm_gradient" class="classattr">
                                        <input id="descent.numerical_norm_gradient-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">numerical_norm_gradient</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.numerical_norm_gradient-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.numerical_norm_gradient"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.numerical_norm_gradient-195"><a href="#descent.numerical_norm_gradient-195"><span class="linenos">195</span></a>    <span class="k">def</span> <span class="nf">numerical_norm_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
</span><span id="descent.numerical_norm_gradient-196"><a href="#descent.numerical_norm_gradient-196"><span class="linenos">196</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.numerical_norm_gradient-197"><a href="#descent.numerical_norm_gradient-197"><span class="linenos">197</span></a><span class="sd">        Compute the numerical gradient of L_norm with respect to w.\\</span>
</span><span id="descent.numerical_norm_gradient-198"><a href="#descent.numerical_norm_gradient-198"><span class="linenos">198</span></a><span class="sd">        Uses the derivative scheme from Wirtinger calculus. See Torch documentation.</span>
</span><span id="descent.numerical_norm_gradient-199"><a href="#descent.numerical_norm_gradient-199"><span class="linenos">199</span></a>
</span><span id="descent.numerical_norm_gradient-200"><a href="#descent.numerical_norm_gradient-200"><span class="linenos">200</span></a><span class="sd">        - `epsilon`: infinitesimal step in parameters</span>
</span><span id="descent.numerical_norm_gradient-201"><a href="#descent.numerical_norm_gradient-201"><span class="linenos">201</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.numerical_norm_gradient-202"><a href="#descent.numerical_norm_gradient-202"><span class="linenos">202</span></a>        <span class="n">w</span> <span class="o">=</span> <span class="n">splice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">all</span><span class="p">)</span> <span class="c1"># save weights</span>
</span><span id="descent.numerical_norm_gradient-203"><a href="#descent.numerical_norm_gradient-203"><span class="linenos">203</span></a>        <span class="n">w_patched</span> <span class="o">=</span> <span class="n">patch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span id="descent.numerical_norm_gradient-204"><a href="#descent.numerical_norm_gradient-204"><span class="linenos">204</span></a>        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="descent.numerical_norm_gradient-205"><a href="#descent.numerical_norm_gradient-205"><span class="linenos">205</span></a>        
</span><span id="descent.numerical_norm_gradient-206"><a href="#descent.numerical_norm_gradient-206"><span class="linenos">206</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">)):</span>
</span><span id="descent.numerical_norm_gradient-207"><a href="#descent.numerical_norm_gradient-207"><span class="linenos">207</span></a>            <span class="c1"># calculate positive- and negative-moved *spliced* weights</span>
</span><span id="descent.numerical_norm_gradient-208"><a href="#descent.numerical_norm_gradient-208"><span class="linenos">208</span></a>            <span class="c1"># not the most optimal way to do things, thank ChatGPT for that</span>
</span><span id="descent.numerical_norm_gradient-209"><a href="#descent.numerical_norm_gradient-209"><span class="linenos">209</span></a>            <span class="n">w_pos</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="descent.numerical_norm_gradient-210"><a href="#descent.numerical_norm_gradient-210"><span class="linenos">210</span></a>            <span class="n">w_neg</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="descent.numerical_norm_gradient-211"><a href="#descent.numerical_norm_gradient-211"><span class="linenos">211</span></a>            <span class="n">w_pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
</span><span id="descent.numerical_norm_gradient-212"><a href="#descent.numerical_norm_gradient-212"><span class="linenos">212</span></a>            <span class="n">w_neg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
</span><span id="descent.numerical_norm_gradient-213"><a href="#descent.numerical_norm_gradient-213"><span class="linenos">213</span></a>
</span><span id="descent.numerical_norm_gradient-214"><a href="#descent.numerical_norm_gradient-214"><span class="linenos">214</span></a>            <span class="c1"># update NN weights (*patched* ofc) and calculate the loss for both positive and negative</span>
</span><span id="descent.numerical_norm_gradient-215"><a href="#descent.numerical_norm_gradient-215"><span class="linenos">215</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_pos</span><span class="p">))</span>
</span><span id="descent.numerical_norm_gradient-216"><a href="#descent.numerical_norm_gradient-216"><span class="linenos">216</span></a>            <span class="n">loss_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="descent.numerical_norm_gradient-217"><a href="#descent.numerical_norm_gradient-217"><span class="linenos">217</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">patch</span><span class="p">(</span><span class="n">w_neg</span><span class="p">))</span>
</span><span id="descent.numerical_norm_gradient-218"><a href="#descent.numerical_norm_gradient-218"><span class="linenos">218</span></a>            <span class="n">loss_neg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span>
</span><span id="descent.numerical_norm_gradient-219"><a href="#descent.numerical_norm_gradient-219"><span class="linenos">219</span></a>            
</span><span id="descent.numerical_norm_gradient-220"><a href="#descent.numerical_norm_gradient-220"><span class="linenos">220</span></a>            <span class="c1"># restore the original *patched* weights after computation</span>
</span><span id="descent.numerical_norm_gradient-221"><a href="#descent.numerical_norm_gradient-221"><span class="linenos">221</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w_patched</span><span class="p">)</span> 
</span><span id="descent.numerical_norm_gradient-222"><a href="#descent.numerical_norm_gradient-222"><span class="linenos">222</span></a>
</span><span id="descent.numerical_norm_gradient-223"><a href="#descent.numerical_norm_gradient-223"><span class="linenos">223</span></a>            <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_pos</span> <span class="o">-</span> <span class="n">loss_neg</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="p">)</span>  <span class="c1"># Central difference approximation</span>
</span><span id="descent.numerical_norm_gradient-224"><a href="#descent.numerical_norm_gradient-224"><span class="linenos">224</span></a>
</span><span id="descent.numerical_norm_gradient-225"><a href="#descent.numerical_norm_gradient-225"><span class="linenos">225</span></a>        <span class="k">return</span> <span class="n">patch</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">*</span><span class="mf">0.5</span>
</span></pre></div>


            <div class="docstring"><p>Compute the numerical gradient of L_norm with respect to w.\
Uses the derivative scheme from Wirtinger calculus. See Torch documentation.</p>

<ul>
<li><code><a href="#descent.epsilon">epsilon</a></code>: infinitesimal step in parameters</li>
</ul>
</div>


                            </div>
                            <div id="descent.normalise_params" class="classattr">
                                        <input id="descent.normalise_params-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">normalise_params</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span>, </span><span class="param"><span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-08</span>, </span><span class="param"><span class="n">maxiters</span><span class="o">=</span><span class="mi">1000</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.normalise_params-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.normalise_params"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.normalise_params-227"><a href="#descent.normalise_params-227"><span class="linenos">227</span></a>    <span class="k">def</span> <span class="nf">normalise_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
</span><span id="descent.normalise_params-228"><a href="#descent.normalise_params-228"><span class="linenos">228</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.normalise_params-229"><a href="#descent.normalise_params-229"><span class="linenos">229</span></a><span class="sd">        Attempts to find a set of network parameters which represent a normalised wave function.\\</span>
</span><span id="descent.normalise_params-230"><a href="#descent.normalise_params-230"><span class="linenos">230</span></a><span class="sd">        This is done by optimizing the loss `L = (1-&lt;psi|psi&gt;)**2` with gradient descent.</span>
</span><span id="descent.normalise_params-231"><a href="#descent.normalise_params-231"><span class="linenos">231</span></a>
</span><span id="descent.normalise_params-232"><a href="#descent.normalise_params-232"><span class="linenos">232</span></a><span class="sd">        - `tolerance`: controls how precise the convergence should be</span>
</span><span id="descent.normalise_params-233"><a href="#descent.normalise_params-233"><span class="linenos">233</span></a><span class="sd">        - `maxiters`: manual break for the training loop if it goes into too many steps</span>
</span><span id="descent.normalise_params-234"><a href="#descent.normalise_params-234"><span class="linenos">234</span></a>
</span><span id="descent.normalise_params-235"><a href="#descent.normalise_params-235"><span class="linenos">235</span></a><span class="sd">        Fun fact: normaliSe is British, normaliZe is American English</span>
</span><span id="descent.normalise_params-236"><a href="#descent.normalise_params-236"><span class="linenos">236</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.normalise_params-237"><a href="#descent.normalise_params-237"><span class="linenos">237</span></a>        <span class="c1"># counter for manual loop break</span>
</span><span id="descent.normalise_params-238"><a href="#descent.normalise_params-238"><span class="linenos">238</span></a>        <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="descent.normalise_params-239"><a href="#descent.normalise_params-239"><span class="linenos">239</span></a>
</span><span id="descent.normalise_params-240"><a href="#descent.normalise_params-240"><span class="linenos">240</span></a>        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_Lnorm</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">:</span>
</span><span id="descent.normalise_params-241"><a href="#descent.normalise_params-241"><span class="linenos">241</span></a>            <span class="c1"># 1. calculate the gradient</span>
</span><span id="descent.normalise_params-242"><a href="#descent.normalise_params-242"><span class="linenos">242</span></a>            <span class="c1">#grad = self.numerical_norm_gradient()</span>
</span><span id="descent.normalise_params-243"><a href="#descent.normalise_params-243"><span class="linenos">243</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent.normalise_params-244"><a href="#descent.normalise_params-244"><span class="linenos">244</span></a>
</span><span id="descent.normalise_params-245"><a href="#descent.normalise_params-245"><span class="linenos">245</span></a>            <span class="c1"># 3. update weights by gradient descent</span>
</span><span id="descent.normalise_params-246"><a href="#descent.normalise_params-246"><span class="linenos">246</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span>
</span><span id="descent.normalise_params-247"><a href="#descent.normalise_params-247"><span class="linenos">247</span></a>                                <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
</span><span id="descent.normalise_params-248"><a href="#descent.normalise_params-248"><span class="linenos">248</span></a>
</span><span id="descent.normalise_params-249"><a href="#descent.normalise_params-249"><span class="linenos">249</span></a>            <span class="c1"># 4. breaking conditions</span>
</span><span id="descent.normalise_params-250"><a href="#descent.normalise_params-250"><span class="linenos">250</span></a>            <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="descent.normalise_params-251"><a href="#descent.normalise_params-251"><span class="linenos">251</span></a>            <span class="k">if</span> <span class="n">iters</span> <span class="o">&gt;=</span> <span class="n">maxiters</span><span class="p">:</span>
</span><span id="descent.normalise_params-252"><a href="#descent.normalise_params-252"><span class="linenos">252</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loop broken after &quot;</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="s2">&quot; iterations&quot;</span><span class="p">)</span>
</span><span id="descent.normalise_params-253"><a href="#descent.normalise_params-253"><span class="linenos">253</span></a>                <span class="k">break</span>
</span><span id="descent.normalise_params-254"><a href="#descent.normalise_params-254"><span class="linenos">254</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parametric normalization complete, norm = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Attempts to find a set of network parameters which represent a normalised wave function.\
This is done by optimizing the loss <code>L = (1-&lt;psi|psi&gt;)**2</code> with gradient descent.</p>

<ul>
<li><code>tolerance</code>: controls how precise the convergence should be</li>
<li><code>maxiters</code>: manual break for the training loop if it goes into too many steps</li>
</ul>

<p>Fun fact: normaliSe is British, normaliZe is American English</p>
</div>


                            </div>
                            <div id="descent.refactor_Lnorm" class="classattr">
                                        <input id="descent.refactor_Lnorm-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">refactor_Lnorm</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">gradLnorm</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.refactor_Lnorm-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.refactor_Lnorm"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.refactor_Lnorm-256"><a href="#descent.refactor_Lnorm-256"><span class="linenos">256</span></a>    <span class="k">def</span> <span class="nf">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradLnorm</span><span class="p">):</span>
</span><span id="descent.refactor_Lnorm-257"><a href="#descent.refactor_Lnorm-257"><span class="linenos">257</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.refactor_Lnorm-258"><a href="#descent.refactor_Lnorm-258"><span class="linenos">258</span></a><span class="sd">        Rescales the `gradLnorm` so that its norm has an upper limit corresponding to the maximum norm of the energy gradient in the whole simulation.\\</span>
</span><span id="descent.refactor_Lnorm-259"><a href="#descent.refactor_Lnorm-259"><span class="linenos">259</span></a><span class="sd">        This means that, if its norm is bigger, then reduce it to that limit, otherwise do nothing.</span>
</span><span id="descent.refactor_Lnorm-260"><a href="#descent.refactor_Lnorm-260"><span class="linenos">260</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.refactor_Lnorm-261"><a href="#descent.refactor_Lnorm-261"><span class="linenos">261</span></a>        <span class="c1">#NOTE: garbage code</span>
</span><span id="descent.refactor_Lnorm-262"><a href="#descent.refactor_Lnorm-262"><span class="linenos">262</span></a>            <span class="c1">#self.normSF.append(np.linalg.norm(self.gradient() / (1. + self.eta*np.linalg.norm(self.gradient())) ) if self.taming else np.linalg.norm(self.gradient()))</span>
</span><span id="descent.refactor_Lnorm-263"><a href="#descent.refactor_Lnorm-263"><span class="linenos">263</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad / (1. + self.eta*np.linalg.norm(self.sampler.Lnorm_grad)) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent.refactor_Lnorm-264"><a href="#descent.refactor_Lnorm-264"><span class="linenos">264</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(gradLnorm * np.linalg.norm(self.sampler.F) / np.linalg.norm(gradLnorm) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent.refactor_Lnorm-265"><a href="#descent.refactor_Lnorm-265"><span class="linenos">265</span></a>            <span class="c1">#self.normLnorm.append(np.linalg.norm(self.sampler.Lnorm_grad * max(self.normF) / np.linalg.norm(self.sampler.Lnorm_grad) ) if self.taming else np.linalg.norm(self.sampler.Lnorm_grad))</span>
</span><span id="descent.refactor_Lnorm-266"><a href="#descent.refactor_Lnorm-266"><span class="linenos">266</span></a>            
</span><span id="descent.refactor_Lnorm-267"><a href="#descent.refactor_Lnorm-267"><span class="linenos">267</span></a>            <span class="c1">#grad = grad / (1 + self.eta*np.linalg.norm(grad))</span>
</span><span id="descent.refactor_Lnorm-268"><a href="#descent.refactor_Lnorm-268"><span class="linenos">268</span></a>            <span class="c1">#gradLnorm = gradLnorm / (1 + self.eta*np.linalg.norm(gradLnorm))</span>
</span><span id="descent.refactor_Lnorm-269"><a href="#descent.refactor_Lnorm-269"><span class="linenos">269</span></a>            <span class="c1">#gradLnorm = gradLnorm * max(self.normF) / np.linalg.norm(gradLnorm)</span>
</span><span id="descent.refactor_Lnorm-270"><a href="#descent.refactor_Lnorm-270"><span class="linenos">270</span></a>        <span class="c1"># hopefully you skipped this</span>
</span><span id="descent.refactor_Lnorm-271"><a href="#descent.refactor_Lnorm-271"><span class="linenos">271</span></a>
</span><span id="descent.refactor_Lnorm-272"><a href="#descent.refactor_Lnorm-272"><span class="linenos">272</span></a>        <span class="c1"># upper limit to the norm</span>
</span><span id="descent.refactor_Lnorm-273"><a href="#descent.refactor_Lnorm-273"><span class="linenos">273</span></a>        <span class="n">upper_limit</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="p">)</span>
</span><span id="descent.refactor_Lnorm-274"><a href="#descent.refactor_Lnorm-274"><span class="linenos">274</span></a>        <span class="c1">#upper_limit= self.normF[-1]</span>
</span><span id="descent.refactor_Lnorm-275"><a href="#descent.refactor_Lnorm-275"><span class="linenos">275</span></a>        <span class="c1">#gradLnorm = np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S))) @ gradLnorm</span>
</span><span id="descent.refactor_Lnorm-276"><a href="#descent.refactor_Lnorm-276"><span class="linenos">276</span></a>
</span><span id="descent.refactor_Lnorm-277"><a href="#descent.refactor_Lnorm-277"><span class="linenos">277</span></a>        <span class="c1"># see if it&#39;s bigger</span>
</span><span id="descent.refactor_Lnorm-278"><a href="#descent.refactor_Lnorm-278"><span class="linenos">278</span></a>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">upper_limit</span><span class="p">:</span>
</span><span id="descent.refactor_Lnorm-279"><a href="#descent.refactor_Lnorm-279"><span class="linenos">279</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="n">upper_limit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="descent.refactor_Lnorm-280"><a href="#descent.refactor_Lnorm-280"><span class="linenos">280</span></a>
</span><span id="descent.refactor_Lnorm-281"><a href="#descent.refactor_Lnorm-281"><span class="linenos">281</span></a>        <span class="c1">#return np.linalg.pinv(self.sampler.S + self._epsilon*np.eye(len(self.sampler.S)))@gradLnorm</span>
</span><span id="descent.refactor_Lnorm-282"><a href="#descent.refactor_Lnorm-282"><span class="linenos">282</span></a>        <span class="k">return</span> <span class="n">gradLnorm</span>
</span></pre></div>


            <div class="docstring"><p>Rescales the <code>gradLnorm</code> so that its norm has an upper limit corresponding to the maximum norm of the energy gradient in the whole simulation.\
This means that, if its norm is bigger, then reduce it to that limit, otherwise do nothing.</p>
</div>


                            </div>
                            <div id="descent.lagrange_gradient" class="classattr">
                                        <input id="descent.lagrange_gradient-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">lagrange_gradient</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">s</span><span class="o">=</span><span class="mi">0</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.lagrange_gradient-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.lagrange_gradient"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.lagrange_gradient-286"><a href="#descent.lagrange_gradient-286"><span class="linenos">286</span></a>    <span class="k">def</span> <span class="nf">lagrange_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="descent.lagrange_gradient-287"><a href="#descent.lagrange_gradient-287"><span class="linenos">287</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.lagrange_gradient-288"><a href="#descent.lagrange_gradient-288"><span class="linenos">288</span></a><span class="sd">        Performs an update of parameters that conserves the norm using Lagrange multipler(s).</span>
</span><span id="descent.lagrange_gradient-289"><a href="#descent.lagrange_gradient-289"><span class="linenos">289</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.lagrange_gradient-290"><a href="#descent.lagrange_gradient-290"><span class="linenos">290</span></a>        <span class="c1"># LHS of the EoM</span>
</span><span id="descent.lagrange_gradient-291"><a href="#descent.lagrange_gradient-291"><span class="linenos">291</span></a>        <span class="c1"># make the matrix</span>
</span><span id="descent.lagrange_gradient-292"><a href="#descent.lagrange_gradient-292"><span class="linenos">292</span></a>        <span class="c1"># apply the regularization only on the S matrix?</span>
</span><span id="descent.lagrange_gradient-293"><a href="#descent.lagrange_gradient-293"><span class="linenos">293</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">S</span><span class="p">)),</span> 
</span><span id="descent.lagrange_gradient-294"><a href="#descent.lagrange_gradient-294"><span class="linenos">294</span></a>                             <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent.lagrange_gradient-295"><a href="#descent.lagrange_gradient-295"><span class="linenos">295</span></a>                             <span class="p">))</span>
</span><span id="descent.lagrange_gradient-296"><a href="#descent.lagrange_gradient-296"><span class="linenos">296</span></a>        <span class="c1"># M = np.column_stack((self.sampler.S, </span>
</span><span id="descent.lagrange_gradient-297"><a href="#descent.lagrange_gradient-297"><span class="linenos">297</span></a>        <span class="c1">#                      self.sampler.Lnorm_grad))</span>
</span><span id="descent.lagrange_gradient-298"><a href="#descent.lagrange_gradient-298"><span class="linenos">298</span></a>        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="descent.lagrange_gradient-299"><a href="#descent.lagrange_gradient-299"><span class="linenos">299</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">,</span> <span class="mf">0.</span>
</span><span id="descent.lagrange_gradient-300"><a href="#descent.lagrange_gradient-300"><span class="linenos">300</span></a>            <span class="p">)))</span>
</span><span id="descent.lagrange_gradient-301"><a href="#descent.lagrange_gradient-301"><span class="linenos">301</span></a>
</span><span id="descent.lagrange_gradient-302"><a href="#descent.lagrange_gradient-302"><span class="linenos">302</span></a>        <span class="c1"># or regularization of the WHOLE matrix?</span>
</span><span id="descent.lagrange_gradient-303"><a href="#descent.lagrange_gradient-303"><span class="linenos">303</span></a>        <span class="c1"># M = M + self.epsilon(s)*np.eye(len(M))</span>
</span><span id="descent.lagrange_gradient-304"><a href="#descent.lagrange_gradient-304"><span class="linenos">304</span></a>        
</span><span id="descent.lagrange_gradient-305"><a href="#descent.lagrange_gradient-305"><span class="linenos">305</span></a>        <span class="c1"># RHS of the EoM</span>
</span><span id="descent.lagrange_gradient-306"><a href="#descent.lagrange_gradient-306"><span class="linenos">306</span></a>        <span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">F</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
</span><span id="descent.lagrange_gradient-307"><a href="#descent.lagrange_gradient-307"><span class="linenos">307</span></a>
</span><span id="descent.lagrange_gradient-308"><a href="#descent.lagrange_gradient-308"><span class="linenos">308</span></a>        <span class="c1"># solution</span>
</span><span id="descent.lagrange_gradient-309"><a href="#descent.lagrange_gradient-309"><span class="linenos">309</span></a>        <span class="c1"># don&#39;t forget to leave out the last element because that&#39;s the multipler</span>
</span><span id="descent.lagrange_gradient-310"><a href="#descent.lagrange_gradient-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">@</span> <span class="n">F</span><span class="p">)</span> <span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Performs an update of parameters that conserves the norm using Lagrange multipler(s).</p>
</div>


                            </div>
                            <div id="descent.optimize" class="classattr">
                                        <input id="descent.optimize-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimize</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">precision</span><span class="o">=</span><span class="mf">1e-10</span>, </span><span class="param"><span class="n">miniter</span><span class="o">=</span><span class="mi">100</span>, </span><span class="param"><span class="n">maxiter</span><span class="o">=</span><span class="mi">10000</span>, </span><span class="param"><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.optimize-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.optimize"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.optimize-312"><a href="#descent.optimize-312"><span class="linenos">312</span></a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">precision</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">miniter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="descent.optimize-313"><a href="#descent.optimize-313"><span class="linenos">313</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.optimize-314"><a href="#descent.optimize-314"><span class="linenos">314</span></a><span class="sd">        Performs the gradient descent algorithm.</span>
</span><span id="descent.optimize-315"><a href="#descent.optimize-315"><span class="linenos">315</span></a>
</span><span id="descent.optimize-316"><a href="#descent.optimize-316"><span class="linenos">316</span></a><span class="sd">        - `precision`: convergence criterion for the loss function</span>
</span><span id="descent.optimize-317"><a href="#descent.optimize-317"><span class="linenos">317</span></a><span class="sd">        - `miniter`: minimum number of iterations</span>
</span><span id="descent.optimize-318"><a href="#descent.optimize-318"><span class="linenos">318</span></a><span class="sd">        - `maxiter`: maximum number of iterations</span>
</span><span id="descent.optimize-319"><a href="#descent.optimize-319"><span class="linenos">319</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.optimize-320"><a href="#descent.optimize-320"><span class="linenos">320</span></a>        <span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># iterations</span>
</span><span id="descent.optimize-321"><a href="#descent.optimize-321"><span class="linenos">321</span></a>        <span class="c1">#params = np.real(self.states[0].copy()) + 1j*np.zeros(len(self.states[0])) # initial parameters</span>
</span><span id="descent.optimize-322"><a href="#descent.optimize-322"><span class="linenos">322</span></a>        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
</span><span id="descent.optimize-323"><a href="#descent.optimize-323"><span class="linenos">323</span></a>
</span><span id="descent.optimize-324"><a href="#descent.optimize-324"><span class="linenos">324</span></a>        <span class="c1"># main loop</span>
</span><span id="descent.optimize-325"><a href="#descent.optimize-325"><span class="linenos">325</span></a>        <span class="k">while</span> <span class="nb">iter</span> <span class="o">&lt;=</span> <span class="n">maxiter</span><span class="p">:</span>
</span><span id="descent.optimize-326"><a href="#descent.optimize-326"><span class="linenos">326</span></a>            <span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="descent.optimize-327"><a href="#descent.optimize-327"><span class="linenos">327</span></a>
</span><span id="descent.optimize-328"><a href="#descent.optimize-328"><span class="linenos">328</span></a>            <span class="c1"># if self.zigzag:</span>
</span><span id="descent.optimize-329"><a href="#descent.optimize-329"><span class="linenos">329</span></a>            <span class="c1">#     self.normalise_params()</span>
</span><span id="descent.optimize-330"><a href="#descent.optimize-330"><span class="linenos">330</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="descent.optimize-331"><a href="#descent.optimize-331"><span class="linenos">331</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="nb">iter</span><span class="p">)</span> <span class="c1"># gradient</span>
</span><span id="descent.optimize-332"><a href="#descent.optimize-332"><span class="linenos">332</span></a>            <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span>
</span><span id="descent.optimize-333"><a href="#descent.optimize-333"><span class="linenos">333</span></a>
</span><span id="descent.optimize-334"><a href="#descent.optimize-334"><span class="linenos">334</span></a>            <span class="c1">#do you refactor?</span>
</span><span id="descent.optimize-335"><a href="#descent.optimize-335"><span class="linenos">335</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactoring</span><span class="p">:</span>
</span><span id="descent.optimize-336"><a href="#descent.optimize-336"><span class="linenos">336</span></a>                <span class="n">gradLnorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="n">gradLnorm</span><span class="p">)</span>
</span><span id="descent.optimize-337"><a href="#descent.optimize-337"><span class="linenos">337</span></a>            <span class="c1">#do you penalize?</span>
</span><span id="descent.optimize-338"><a href="#descent.optimize-338"><span class="linenos">338</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_norm</span><span class="p">:</span> <span class="c1"># if you&#39;re also normalizing, penalize the norm in the gradient</span>
</span><span id="descent.optimize-339"><a href="#descent.optimize-339"><span class="linenos">339</span></a>                <span class="n">grad</span> <span class="o">+=</span> <span class="n">gradLnorm</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">Lnorm_importance</span><span class="p">(</span><span class="nb">iter</span><span class="p">)</span>
</span><span id="descent.optimize-340"><a href="#descent.optimize-340"><span class="linenos">340</span></a>            <span class="c1"># if not either of those things, you are probably lagranging</span>
</span><span id="descent.optimize-341"><a href="#descent.optimize-341"><span class="linenos">341</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange</span><span class="p">:</span>
</span><span id="descent.optimize-342"><a href="#descent.optimize-342"><span class="linenos">342</span></a>                <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lagrange_gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">)</span>
</span><span id="descent.optimize-343"><a href="#descent.optimize-343"><span class="linenos">343</span></a>
</span><span id="descent.optimize-344"><a href="#descent.optimize-344"><span class="linenos">344</span></a>            <span class="n">params</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># descent</span>
</span><span id="descent.optimize-345"><a href="#descent.optimize-345"><span class="linenos">345</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="c1"># change all the elements of the sampler (network, S, F, etc...)</span>
</span><span id="descent.optimize-346"><a href="#descent.optimize-346"><span class="linenos">346</span></a>
</span><span id="descent.optimize-347"><a href="#descent.optimize-347"><span class="linenos">347</span></a>            <span class="c1"># accumulate things</span>
</span><span id="descent.optimize-348"><a href="#descent.optimize-348"><span class="linenos">348</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span> <span class="c1"># append the loss function</span>
</span><span id="descent.optimize-349"><a href="#descent.optimize-349"><span class="linenos">349</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="descent.optimize-350"><a href="#descent.optimize-350"><span class="linenos">350</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">psis</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">psiket</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="descent.optimize-351"><a href="#descent.optimize-351"><span class="linenos">351</span></a>            <span class="c1">#self.normF.append(np.linalg.norm(self.sampler.F))</span>
</span><span id="descent.optimize-352"><a href="#descent.optimize-352"><span class="linenos">352</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normF</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">s</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
</span><span id="descent.optimize-353"><a href="#descent.optimize-353"><span class="linenos">353</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">normLnorm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refactor_Lnorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Lnorm_grad</span><span class="p">)))</span>
</span><span id="descent.optimize-354"><a href="#descent.optimize-354"><span class="linenos">354</span></a>            
</span><span id="descent.optimize-355"><a href="#descent.optimize-355"><span class="linenos">355</span></a>            <span class="c1"># sponsored messages</span>
</span><span id="descent.optimize-356"><a href="#descent.optimize-356"><span class="linenos">356</span></a>            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">iter</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="descent.optimize-357"><a href="#descent.optimize-357"><span class="linenos">357</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="s2">&quot;... energy: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="descent.optimize-358"><a href="#descent.optimize-358"><span class="linenos">358</span></a>            
</span><span id="descent.optimize-359"><a href="#descent.optimize-359"><span class="linenos">359</span></a>            <span class="c1"># check criteria</span>
</span><span id="descent.optimize-360"><a href="#descent.optimize-360"><span class="linenos">360</span></a>            <span class="k">if</span> <span class="nb">iter</span> <span class="o">&gt;</span> <span class="n">miniter</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">precision</span><span class="p">:</span>
</span><span id="descent.optimize-361"><a href="#descent.optimize-361"><span class="linenos">361</span></a>                <span class="k">break</span>
</span></pre></div>


            <div class="docstring"><p>Performs the gradient descent algorithm.</p>

<ul>
<li><code>precision</code>: convergence criterion for the loss function</li>
<li><code>miniter</code>: minimum number of iterations</li>
<li><code>maxiter</code>: maximum number of iterations</li>
</ul>
</div>


                            </div>
                            <div id="descent.process" class="classattr">
                                        <input id="descent.process-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">process</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">links</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.process-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.process"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.process-364"><a href="#descent.process-364"><span class="linenos">364</span></a>    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">links</span><span class="p">):</span>
</span><span id="descent.process-365"><a href="#descent.process-365"><span class="linenos">365</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.process-366"><a href="#descent.process-366"><span class="linenos">366</span></a><span class="sd">        Calculates the correlation functions at each state of the process.\\</span>
</span><span id="descent.process-367"><a href="#descent.process-367"><span class="linenos">367</span></a><span class="sd">        Correlatoins are written in a `link -&gt; list` dictionary, where the `list` is all the values of the correlation at the lattice `link` during the process.</span>
</span><span id="descent.process-368"><a href="#descent.process-368"><span class="linenos">368</span></a>
</span><span id="descent.process-369"><a href="#descent.process-369"><span class="linenos">369</span></a><span class="sd">        - `links`: an array of tuples representing links on the lattice where the calculation is performed </span>
</span><span id="descent.process-370"><a href="#descent.process-370"><span class="linenos">370</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.process-371"><a href="#descent.process-371"><span class="linenos">371</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="descent.process-372"><a href="#descent.process-372"><span class="linenos">372</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="descent.process-373"><a href="#descent.process-373"><span class="linenos">373</span></a>
</span><span id="descent.process-374"><a href="#descent.process-374"><span class="linenos">374</span></a>        <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
</span><span id="descent.process-375"><a href="#descent.process-375"><span class="linenos">375</span></a>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculating correlation at link&quot;</span><span class="p">,</span> <span class="n">link</span><span class="p">)</span>
</span><span id="descent.process-376"><a href="#descent.process-376"><span class="linenos">376</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="descent.process-377"><a href="#descent.process-377"><span class="linenos">377</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="descent.process-378"><a href="#descent.process-378"><span class="linenos">378</span></a>
</span><span id="descent.process-379"><a href="#descent.process-379"><span class="linenos">379</span></a>            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">))</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
</span><span id="descent.process-380"><a href="#descent.process-380"><span class="linenos">380</span></a>                <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
</span><span id="descent.process-381"><a href="#descent.process-381"><span class="linenos">381</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</span><span id="descent.process-382"><a href="#descent.process-382"><span class="linenos">382</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">correlations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Cvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent.process-383"><a href="#descent.process-383"><span class="linenos">383</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">zcorrelations</span><span class="p">[</span><span class="n">link</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Czvar</span><span class="p">(</span><span class="n">link</span><span class="p">)</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent.process-384"><a href="#descent.process-384"><span class="linenos">384</span></a>                    
</span><span id="descent.process-385"><a href="#descent.process-385"><span class="linenos">385</span></a>                    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Calculates the correlation functions at each state of the process.\
Correlatoins are written in a <code>link -&gt; list</code> dictionary, where the <code>list</code> is all the values of the correlation at the lattice <code>link</code> during the process.</p>

<ul>
<li><code>links</code>: an array of tuples representing links on the lattice where the calculation is performed</li>
</ul>
</div>


                            </div>
                            <div id="descent.nudge" class="classattr">
                                        <input id="descent.nudge-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">nudge</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">phase_nudge</span><span class="o">=</span><span class="mf">1.5707963267948966</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="descent.nudge-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#descent.nudge"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="descent.nudge-388"><a href="#descent.nudge-388"><span class="linenos">388</span></a>    <span class="k">def</span> <span class="nf">nudge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase_nudge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">):</span>
</span><span id="descent.nudge-389"><a href="#descent.nudge-389"><span class="linenos">389</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="descent.nudge-390"><a href="#descent.nudge-390"><span class="linenos">390</span></a><span class="sd">        Corrects the weights of the neural network to be complex, but still have the same energy.\\</span>
</span><span id="descent.nudge-391"><a href="#descent.nudge-391"><span class="linenos">391</span></a><span class="sd">        Uses a root-finding algorithm to find the state whose energy offset is zero. This is equivalent to supervised learning.</span>
</span><span id="descent.nudge-392"><a href="#descent.nudge-392"><span class="linenos">392</span></a>
</span><span id="descent.nudge-393"><a href="#descent.nudge-393"><span class="linenos">393</span></a><span class="sd">        1. Initialize weights: take the trained weights in `states[-1]` and multiply them with `exp(1j*phase_nudge)`.</span>
</span><span id="descent.nudge-394"><a href="#descent.nudge-394"><span class="linenos">394</span></a><span class="sd">        2. Find the root: construct a function `L (w) = |E_0 - E(w)|`, where `E_0` is the pre-trained energy, and find the weights `w` for which this function has a root.</span>
</span><span id="descent.nudge-395"><a href="#descent.nudge-395"><span class="linenos">395</span></a>
</span><span id="descent.nudge-396"><a href="#descent.nudge-396"><span class="linenos">396</span></a><span class="sd">        USE only if you have performed the graident descent by the `descent()` function. Otherwise the results are senseless.</span>
</span><span id="descent.nudge-397"><a href="#descent.nudge-397"><span class="linenos">397</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="descent.nudge-398"><a href="#descent.nudge-398"><span class="linenos">398</span></a>
</span><span id="descent.nudge-399"><a href="#descent.nudge-399"><span class="linenos">399</span></a>        <span class="c1"># 1. nudge the parameters</span>
</span><span id="descent.nudge-400"><a href="#descent.nudge-400"><span class="linenos">400</span></a>        <span class="n">w0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="n">phase_nudge</span><span class="p">)</span>
</span><span id="descent.nudge-401"><a href="#descent.nudge-401"><span class="linenos">401</span></a>
</span><span id="descent.nudge-402"><a href="#descent.nudge-402"><span class="linenos">402</span></a>        <span class="c1"># 2. create the function</span>
</span><span id="descent.nudge-403"><a href="#descent.nudge-403"><span class="linenos">403</span></a>        <span class="k">def</span> <span class="nf">nudgeloss</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="descent.nudge-404"><a href="#descent.nudge-404"><span class="linenos">404</span></a>            <span class="c1"># update the sampler</span>
</span><span id="descent.nudge-405"><a href="#descent.nudge-405"><span class="linenos">405</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">just_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="descent.nudge-406"><a href="#descent.nudge-406"><span class="linenos">406</span></a>
</span><span id="descent.nudge-407"><a href="#descent.nudge-407"><span class="linenos">407</span></a>            <span class="c1"># return the value</span>
</span><span id="descent.nudge-408"><a href="#descent.nudge-408"><span class="linenos">408</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Evar</span><span class="p">()</span><span class="o">/</span><span class="mf">4.</span><span class="p">)</span>
</span><span id="descent.nudge-409"><a href="#descent.nudge-409"><span class="linenos">409</span></a>        
</span><span id="descent.nudge-410"><a href="#descent.nudge-410"><span class="linenos">410</span></a>        <span class="c1"># 3. run the root finder</span>
</span><span id="descent.nudge-411"><a href="#descent.nudge-411"><span class="linenos">411</span></a>        <span class="n">nudged_weights</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">newton</span><span class="p">(</span><span class="n">nudgeloss</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">maxiter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)</span>
</span><span id="descent.nudge-412"><a href="#descent.nudge-412"><span class="linenos">412</span></a>
</span><span id="descent.nudge-413"><a href="#descent.nudge-413"><span class="linenos">413</span></a>        <span class="k">return</span> <span class="n">nudged_weights</span>
</span></pre></div>


            <div class="docstring"><p>Corrects the weights of the neural network to be complex, but still have the same energy.\
Uses a root-finding algorithm to find the state whose energy offset is zero. This is equivalent to supervised learning.</p>

<ol>
<li>Initialize weights: take the trained weights in <code>states[-1]</code> and multiply them with <code>exp(1j*phase_nudge)</code>.</li>
<li>Find the root: construct a function <code>L (w) = |E_0 - E(w)|</code>, where <code>E_0</code> is the pre-trained energy, and find the weights <code>w</code> for which this function has a root.</li>
</ol>

<p>USE only if you have performed the graident descent by the <code><a href="#descent">descent()</a></code> function. Otherwise the results are senseless.</p>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>